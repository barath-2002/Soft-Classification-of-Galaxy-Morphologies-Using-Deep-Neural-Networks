{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade keras"
      ],
      "metadata": {
        "id": "_DjuGNZkbYgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88215b8d-f6af-4b03-8e62-e068f98fc57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.2.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Collecting namex (from keras)\n",
            "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\n",
            "Collecting optree (from keras)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.10.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Installing collected packages: namex, optree, keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.2.0 namex-0.0.7 optree-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5uy7-mNEmy5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization, GlobalMaxPooling2D\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import MeanSquaredError, RootMeanSquaredError\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.models import load_model\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from tensorflow.keras.callbacks import Callback, CSVLogger"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnk0flupHiTh",
        "outputId": "cae8c718-067b-48e7-8831-6df1f7421c08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "l1GIDHKb7tNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/GalaxyZoo/compressed_img_target.npz'\n",
        "loaded_arrays = np.load(data_path)\n",
        "\n",
        "with tqdm(total=1) as pbar:\n",
        "    training_images = np.load(data_path, allow_pickle=True)  # allow_pickle=True is necessary for loading npz files with Python 3.7+\n",
        "    pbar.update(1) # Update progress bar\n",
        "\n",
        "# Retrieve the arrays\n",
        "train = loaded_arrays['images']\n",
        "target = loaded_arrays['target']\n",
        "\n",
        "print(\"Shape of the array of training images:\", train.shape)"
      ],
      "metadata": {
        "id": "7dC7WayuJ-zD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e29a0441-3c16-4b25-d7bc-70d86d7e7d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 118.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the array of training images: (61578, 64, 64, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First split to separate out the test set\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(train, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize pixel values to range [0, 1]\n",
        "X_temp = X_temp.astype('float32') / 255\n",
        "X_test = X_test.astype('float32') / 255\n",
        "y_temp = y_temp.astype('float32')\n",
        "y_test = y_test.astype('float32')\n",
        "\n",
        "# Second split to divide the remaining data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "sFwMkPfUg5-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image augmentation"
      ],
      "metadata": {
        "id": "HK8Gzjx6k8gA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialising the ImageDataGenerator with specified augmentations\n",
        "training_data_generator = ImageDataGenerator(\n",
        "    zoom_range=0.10,  # Randomly increase or decrease the size of the image by up to 10%\n",
        "    rotation_range=25,  # Randomly rotate the image between -25 to 25 degrees\n",
        "    width_shift_range=0.05,  # Shift the image along its width by up to +/- 5%\n",
        "    height_shift_range=0.05,  # Shift the image along its height by up to +/- 5%\n",
        "    fill_mode='nearest',  # Fill in newly created pixels after a shift or rotation\n",
        ")\n",
        "\n",
        "# Creating a generator that will be used for training\n",
        "train_generator = training_data_generator.flow(\n",
        "    X_train, y_train,\n",
        "    batch_size=128\n",
        ")"
      ],
      "metadata": {
        "id": "XHmne9xAaWV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constrained loss function"
      ],
      "metadata": {
        "id": "Nc32Y5wOigVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss(y_true, y_pred):\n",
        "    lamda = 0.001\n",
        "    c1 = tf.abs(tf.reduce_sum(y_pred[:, 0:3], axis=1) - 1) # Class 1 constraint\n",
        "    c2 = tf.abs(tf.reduce_sum(y_pred[:, 13:15], axis=1) - 1) # Class 6 constraint\n",
        "    c3 = tf.abs((y_pred[:, 3] + y_pred[:, 4]) - y_pred[:, 1]) # Class 2 constraint\n",
        "    c4 = tf.abs((y_pred[:, 15] + y_pred[:, 16] + y_pred[:, 17]) - y_pred[:, 10]) # Class 7 constraint\n",
        "    c5 = tf.abs(tf.reduce_sum(y_pred[:, 18:25], axis=1) - y_pred[:, 13]) # Class 8 constraint\n",
        "    c6 = tf.abs(tf.reduce_sum(y_pred[:, 25:28], axis=1) - y_pred[:, 3]) # Class 9 constraint\n",
        "    c7 = tf.abs(tf.reduce_sum(y_pred[:, 5:7], axis=1) - y_pred[:, 4]) # Class 3 constraint\n",
        "    c8 = tf.abs(tf.reduce_sum(y_pred[:, 7:9], axis=1) - y_pred[:, 4]) # Class 4 constraint\n",
        "    c9 = tf.abs(tf.reduce_sum(y_pred[:, 28:31], axis=1) - y_pred[:, 7]) # Class 10 constraint\n",
        "    c10 = tf.abs(tf.reduce_sum(y_pred[:, 9:13], axis=1) - y_pred[:, 7] - y_pred[:, 8]) # Class 5 constraint\n",
        "    c11 = tf.abs(tf.reduce_sum(y_pred[:, 31:37], axis=1) - y_pred[:, 7]) # Class 11 constraint\n",
        "\n",
        "    c_loss = lamda*(c1+c2+c3+c4+c5+c6+c7+c8+c9+c10+c11)\n",
        "\n",
        "    # Base loss (mean squared error)\n",
        "    mse_loss = tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "\n",
        "    # Combine base loss with constraints\n",
        "    return mse_loss + c_loss"
      ],
      "metadata": {
        "id": "v5YOkQQYlhzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformer"
      ],
      "metadata": {
        "id": "KvBzSVs6DjI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure hyperparameters for ViT architecture"
      ],
      "metadata": {
        "id": "v08uKOmTEB0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "image_size = 64  # Keep image size to 64x64\n",
        "patch_size = 8  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64 # Size of embedding vector\n",
        "num_heads = 8 # Number of attention heads per transformer\n",
        "transformer_units = [projection_dim * 2, projection_dim,]  # Size of the MLP in transformer\n",
        "transformer_layers = 12 # Number of tansformer layers in ViT\n",
        "mlp_head_units = [64, 64,]  # Size of the dense layers of the final classifier"
      ],
      "metadata": {
        "id": "sFntf86StgD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement MLP"
      ],
      "metadata": {
        "id": "C7TG16AWEIRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ay43SV7gEHhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement patch creation as a layer"
      ],
      "metadata": {
        "id": "bQf6yTiWLyxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        input_shape = ops.shape(images)\n",
        "        batch_size = input_shape[0]\n",
        "        height = input_shape[1]\n",
        "        width = input_shape[2]\n",
        "        channels = input_shape[3]\n",
        "        num_patches_h = height // self.patch_size\n",
        "        num_patches_w = width // self.patch_size\n",
        "        patches = keras.ops.image.extract_patches(images, size=self.patch_size)\n",
        "        patches = ops.reshape(\n",
        "            patches,\n",
        "            (\n",
        "                batch_size,\n",
        "                num_patches_h * num_patches_w,\n",
        "                self.patch_size * self.patch_size * channels,\n",
        "            ),\n",
        "        )\n",
        "        return patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"patch_size\": self.patch_size})\n",
        "        return config"
      ],
      "metadata": {
        "id": "EXqpSQoFLfui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example of image as patches"
      ],
      "metadata": {
        "id": "fat-dUFoL3Tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4, 4))\n",
        "image = X_train[np.random.choice(range(X_train.shape[0]))]\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "resized_image = ops.image.resize(\n",
        "    ops.convert_to_tensor([image]), size=(image_size, image_size)\n",
        ")\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = ops.reshape(patch, (patch_size, patch_size, 3))\n",
        "    plt.imshow(ops.convert_to_numpy(patch_img))\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "8DAyojiULj5U",
        "outputId": "9f49bcd3-eae6-427a-fe46-9518703b0321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: 64 X 64\n",
            "Patch size: 8 X 8\n",
            "Patches per image: 64\n",
            "Elements per patch: 192\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 64 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUkAAAFICAYAAADd1gwNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6OUlEQVR4nO2dS6huW3qWv3n/L2vtvc+pcyqaihVCsCKioSRego0QBWPQTvBCCDbFNIU0DMGGpGEjQQSNPS8dQbCTRBBFhYoEREUNXkhSaAVNrFSSyjlnn70u/2XebZzsyv6eMf7xj3UsUDjv05tr/f+cY97Gmuud7/d+xbquqwkhhIhS/r8egBBC/P+MJkkhhEigSVIIIRJokhRCiASaJIUQIoEmSSGESKBJUgghEmiSFEKIBJokhRAiQZ37wd/9LW+75V23c8vvvPOuW77Zb5LLZmbbrvHLjR/ObtO55R//uz99cXw/+pe/3y23deWWm8ovl0Xhlqd1CdbZD5NbPp57t3w6+eW//Q//eXRsP/j9f9wt3z0e3PLD0a9nwFD6MRzby5cv3fL777/vlrfbvVv+5S9/EB2bmdnvePfWLdeVPy83261bHufZLZ/Op2Cdp3Hw62z8Oh8ejm75fPLH+jXdxp+3t549c8sFCsYeHx7dctmEl3ix+O80pd+G4ffvHcL9e83bm9Ytbzb+Op9xrAzjLavILVj47/TD2S1Ps1/HfR9eH2ZmL278WBbs1zD6Y95g7GPvz+FH2/bfaVu//y0eu14d/LX9Jp+69dsrS//lAvdoj/H0U3jNsH6wqvw6a8wL94dwH4meJIUQIoEmSSGESKBJUgghEmRrklXtNaVnz5+75Raa07J4nWSO6Adj6TWHBsvznB9QNI7+swu2NxR+meFHE7UjCzWb48lrQ4eD19Uu8cEHH7rl0+B1kNn8flMenccxWGex+vF2NU5lZH8uMUFrnQu/vQLnYca5Xf3wzcxsGaFbDn45N3yqxDXR49g12O8Z6+V1aWbW4WcnnMdAo0wwL36/zoM/lgs1SOhsG2h6ZmYVtLkR53+ewushxjj4z9XQZ8uaz0j+vDZtOD0sOI8jx1Lk37PUN8vVj2easC1q85HrrsL1UlU8l5EvXUFPkkIIkUCTpBBCJNAkKYQQCfI1ycprJwO0iOPJe//2O/rFwk0F2lvgD4x752Kczl6rqgp4riCVzAZNMqKZUjM5nfw2qFFe4hGaVw+taMS2H7neMvxbVkHboqd0muLeuRgbaE9N489133ttiDrbYqEONeNk0ic5LXnjqwPNkVq3P5YNfIvDEPrgNp0/VtRYqzpfk6ywX7wvDOepht65RDS8dfI/Kwye31DGjIJN2wL9tO38inLUxHr1Y+l73DdV/nMXXzkM8CVzPFVz/bysCzVg/5393vu7c9CTpBBCJNAkKYQQCTRJCiFEgmxNkl6tcfBaTAWtZYDHqY7oiw18W+MEzWHN9/qdz3589JoFmiS0LeqPsZ9RS8yU1aygPoplajHUE+lvMzMboS+dUNc6P0GTXHHuqBXx3BY4djHdL/ChQndlTe0lmqDm+3Dhkx+xhSbJeuDY2LiNMbI/F4Hut9t5zYv7zXrk2PjOqIWnZhq7HmKs8AyOWE8zp72rQd25hfXf260/3vuNr/NPwbpvXkc8ljxWD6jTj31mjWQyPBU9SQohRAJNkkIIkUCTpBBCJMjWJKkPdMgYpKdpWVlzG9abUj8oC68NzVO+Jsna4BoiZKhJeq1onEMdaoSOOmAfTpk+SWqMQa4fNMkZPrmqDutNR3zmjLGx7jUFv7uDzjQv0KOZQxjRyIYZ9d/Q4lhjfYnDwWuQXE+LTNJA44tsZoU2x3XSm5kE62dmwe2tz+rse1/bHdYWm43Q4gdo4xl2QTMzm+gF5mDh6exa7x+Nsa7pc19H9ucSvL/prQ19nvTIhnrjBn7hLeap0+lyNugl9CQphBAJNEkKIUQCTZJCCJFAk6QQQiTIVqhpcKXBuEUToRUuW4ZXmJk9PPrghwLD2TTXheTXDHhxMywolqfouyLIdA3N5AsGPS/plyWXOB79Cx6adA9oNMX1tmP4EmbinzcU8tMgnaLtEEaC/eRxoIA+DeH4GDTQdP7cbru8c9vhczQg7/e+4dkAU32smRVfEASB0Et+cCxf8jB0l+eBy7Hw4ZIvP2CIpkn84thgrC8Y5BE0zUKzvIjRneeeLzPXJ7wwZGAx4QvCHNgsLNhmbCK6gp4khRAigSZJIYRIoElSCCESZGuSH96hmBxN1dcKTb3geK3rUCNjE6GHhwe3PG3y9Y0DQwGgjVCTHCf/eRpXzUKNhkbUcc5syISx0ExuCLxgA7RzTFcL/rwhOCHXcRz57IQwk6Wi2Rq6WsTU2+D6mKGrlk2e7neDwAQG/p6P/px0MMKPkZCGsvRjqxAkO/Z559XM7ExzOHQ8msepN1K7NTOroLvuqC3GLtYINIdTA1yCAgQ05opoktw2Ncqiyjfib/AeIxZ8/SbUP6ugkZkZJV6+D6gbNQITQoivK5okhRAigSZJIYRIkB+6y1AG6FYbgxY0eh1tiWlDWKZmcjodLZfD0Qch0H9G5YfBscsS6iGtpUNBqcdcghrYzIBfaJI1/KFFLDQA4w0aZEWO9yX60etmPFg7hDTQ9Dqf8X0L/Yk1NNOmyNNMuR426QoaivXw8Y1hCMmCIJVN53XP589eZI3NzKxkoDKGEwTkMmMicp4K+CCfPX/mls+ZIQ2852bcsxM9jrhn6FGN/Yw64jVd8U14P9FzSk2UUmxcm4XHd/GfYeBFDnqSFEKIBJokhRAigSZJIYRIUKyx4lEhhBBmpidJIYRIoklSCCESaJIUQogEmiSFECKBJkkhhEiQXXHzqXdu3PKzZz4R+q0XviqjQIXFNIRJNiU+c7vz66yRfv4z//6/XRzfH/3859zytcqADskqJ1TsmJkdUdnACoDjwVcEffFXPoiO7fd9yze4ZQa/HE6+YoWp7mUsWQXFBiOqlVjJ8dXfvIuOzczs+QufxsLKh27j2wmXbM8bSSmaUIXDdPO3333HLX/pf38lOrbPfPptt3zqfQUNa54mJDOVkcIetheuS1+B8+l3Pu2Wv/hLvxwdm5nZ5z77TW75+ODTsipU3BSVv4ZOfVitxESt3a2/L05IPvrqb7yMjm2zQ+IQ9rvFeeY9W1axFCD/M1bWsYrnPFw2z+w6vy6Ob7tNpwQxFSi2jhpJQZwXXt5dr+rTk6QQQiTQJCmEEAk0SQohRIJsTZJd6pgywwTmrkGCTkS36s9eX1rRHY0JKykeH70WdOa6IVYcTtBMI2ksTDdvGj+eKbOrXqA54mtMHi9rf+yaiDbElcwjOuplplebmTW112mo+02L12uZOt5GUor2t17DPqIzJlPeL8EEnP59f6x4Hd7e+s+XgWppdnj0214npJ0fwuSgSzCVncemQydKRtkEKfVmtuB0T0hKD7o7XoCaflMz/cifd94jsbShAnqpIUU/iNtKwOR/wpQgpv7EUoDKMv2ZccxPnf/aOp/8DSGE+AShSVIIIRJokhRCiATZmiT1g2HwOtsHL71+8fzG+ybZRc7MbLvzKcE9k7/H/JRjQk1yt/NevxHp4FXEi8iugFxnh25vl6DudIDPbYH22rKjW0R7oaeth6653fv9TbFBWjM1yQEaGLvclZGU8TN8kvReNkzsvgB16ZsbaJ3UnpFUHjtHQ4mue+bHX6z5zw499GaDX7WgJIn7oIkYOScYUQd0Fc3V/Z4/e+6/xs6G+Dy9rNQwzcLjf6q81nyHjqcp+J6DHka+5+Cx22bo7tQgqXPmoCdJIYRIoElSCCESaJIUQogE2ZpkQa0FnqrTyWtDBfxT77z1qXDj0DnvenY8zOtGaGZ27tPethG6zhF1n+wGaWZBF0PqbLfQfC4xt15rOR+8JrmBNkNtaIp4u6g5Pnvux8Ia4RT09lUFa37hXw3UrBDWLFfsKhgrqo4QaJkY6xY6Vom/+8UUbmfTeKGwwXX4bJ/fUW+LDABWk9cttHzqiyyEN7MtvtNU6OzZ5J3bPcZG/bZBF88R91usNnqBlt+2fh0vbnydeYoNzu1MXyS08LrxGik7hJqZjfA2T3ivMY35c8pr9CQphBAJNEkKIUQCTZJCCJEgW5OkX62FPlBX3j81Qws4HcPcthXaG3Pyuo3Xm5Jcqeukx3GGrjZMYe32CRrkza3X/R7PYRZgjAn6bUufIfS5FWOZ59Av2sOnSh9rbn2vmdkC/9ymQ53+6tfN+tjYtlg3zLrgKbJPOWNjpifDOStosStri82sYV4mdLWa9ckJdjt/rKoaGmjJen+v89UR//B+Tx8qlmP5ohH20FYLnLcWGQF3r5AhEPEhTsiPpGZ8+wRNknmWR3oYce4rjGdaQn3xBN8q/dkcbw56khRCiASaJIUQIoEmSSGESKBJUgghEmSrmGz4s8I0S+M3zcOxgM0JL26qIAQhLLC/BAMsDgdvTH9A4X2z9Z/vI8bZHmGnq/l1xITjGA/3/ns00W9hNl8yzNozzsc8Xn+ZcnldXuzeowET8x74cuQQMfIvKD5g2MIUeaESg+G8fJHDF3QztluV4bGkeX+/88c/GnJ8gbr26+9gBGf48byg+Vbkxc1uizBcJFrwheIlRpzXDczlC4zVLBLYbUNTPUOOT7jPbJNvxOfLvPAlkH8ZbHzxFHmxtKw43hjvuuS/lPvaZp/8DSGE+AShSVIIIRJokhRCiATZmiQDMNkEnjrVzc6bSpdIMTo1R+pYT4HjY2Mw6h3UQwKTspk9RxMqakHPn7/IGhv1Um5pQIAFg1i7liEKZnd3viH9DUy805B/LBlWQrM1QxgY3lrHNDzozZvWn+sxUxui5shrhmHJZQFzeRFqnzye2w3WmTWy3/ru1n+3uxbCUMC4Hjm33db/7IzCgSKyTzF2O68td9Dh196vh6E0sXtiQqHD3fLKr5Nd7hLwnqTezECLETr7OIfvEVjocDp57b4qZSYXQoivK5okhRAigSZJIYRIkP0P+jyn/WlsDkV/0oePYcDF7kqARTHn62rUIOnLZAOjEdJJf4w0Ysc+VtAKj0d4xC5AbYea33Cm5pQ+tmZmZeH1Uip89U1+I7CaPkhoXoeT309u6zbSdKylVgitaNvl+enYLIqaZNiUDDrbGupWNUIoGLRQWP51Rz2TmuSIa7isEHbShZpkySZpJbXCPNV0D526QmMv3qObzh9LaoSxzzDoZsntUmZmL9564ZZHeJV5T0+BJhl6gfmZDh5kzmM56ElSCCESaJIUQogEmiSFECJBsT7F2CSEEJ8w9CQphBAJNEkKIUQCTZJCCJFAk6QQQiTQJCmEEAk0SQohRILsssQ9IqGYprVhbD2dRRGjEftqs0fuCb2yv/pB2CbgNe+88ONj6VzX+W0xnsyKsNRrd3Prf4BysMPBl1q+95uvomP71Lu+dYHh2BToHc3WC4ydMzN78cyXWV5zcv38l37l4u++9Zvedcss6RxmXzbZIfbsWaQskf2ZC8a/bf2x/Xc/94vRsf2hz3/OLdfokd2h9ULBaLJIGwteZ+y7zfYN/+Rf/Vx0bGZmf+Z7vsMtsz892zOEZXFhZNyKYzUuLMfz5/pffuE/Rcf2vd/znW55YSnuoy/FHY7+/opFF1ZFur/7Affsz//Pr0THZmb2+W/7ZrfMbgz39/dumdFpc2T6enj0JbQT7nPGrd0f0es7gp4khRAigSZJIYRIkJ8ChEf1YvL/NpyQyMGk8q4Lk2z4+H/EozLXkaJDGgw7Ek5IPBmQOPL8Rfgv44J/c/gvLRPHL44NSTZMKmkoVcz+tMSOwzD5fxO4/9OY3y2R6dj4j9F2nd/PLWSShjFCFnYRLPj3OJJUH6OBCrJB6k7T+PUGQeCRJOoWK90giaer87PJd/h3n6oH93tFN7+CB9vCf7fPk79Wi8xz2yD1h/fbVPtrqOkgqZXhPXtG98oenTJf3X2YNTYzs7uHO7dM4YFSBZPMT5F/lfnvNaUjplHloCdJIYRIoElSCCESaJIUQogE2Zpkhf/lqa0wxDjU0UIt4Hzy1hJqfrFE7ktQg2SCNaE2MU6hvkHbCnXAsso7fNTRhgFJ0zg2DdKUqWGamTVY51r4E1A1+X//tnu/PSZSM316t4X+G9F5qFO21McyJdMdrFu057RI8aaNpIpIULT8tFhn8YR07aa6or1e0SSZdm9mttY8t/h9Zrh2g+tzwf01Bt1K/YrnKbQATbw2oE9vb0K72kWwX9TGeWzuHh/c8qtXYWeAzcYn3jPZ/nQKOxBcQ0+SQgiRQJOkEEIk0CQphBAJntAt0esTBfSNCgLDNHrtYhxCLWCBcWu7S2skKY5HXyLI7ojUN4fR+7vG0eujZmYL/obQK9oUedoV9cJ58V6uumFHN5ShDeHYhgmldNCXyid4TPvIvr/JBroguz/Gak6pEe+23mtZZZ7at5/58sUC/suyppcOy00oSlKnZCnjU54cqG+WKNujD3Ka/LFaeFGZhWZLej8zJdPySmkwVzPj83OkLJEdClky207pa+lNjj3eSaCksYN/tYS/lR5Is/A+oLfy4zRi0JOkEEIk0CQphBAJNEkKIUSCbE2S/8tTc6qhU42oL43pi9QUuI2Hh9AHdYmioI/Tr+vx8dEt0yfJ+mQzswmaaYP681zNlB5TevlYIx58vgx1FEa9tdANeTxSLFCn2sZrQfSaBcc6ol2tgeLlj1V3xcf6mpud970FnkHovU0H3aoOj13g4A28i1lDMzOz7cYfG1t5Tftf0z96Poca3qn3ftxl4vuA3NHhPCGmjbFtwXmNrJH31Yd3r9zyebwePXZpeyP2c4VmyfyHWHYCPcXcRkzHvIaeJIUQIoEmSSGESKBJUgghEmiSFEKIBNkvbrY1gkk7BrXyxYk3j9NEa2ZWI9y0wzYKLKcosSvjiGJ+BCrUHUymkaABW72QzHDWXGMqhXYGRtCbXcHtPDIt1cLwgj0K++snvH14G6EEWwRYdFv/e2bsxi6iFju1CUKY8y69PcbCt14rjeBcbR0maRR4oRFEUgRm+cvUNXsr8WWJ//yK3/NaMDPrUejAsBjea5eocDCm1b/s6woESrAgZA5fyFmBdeLlZRUJEb7ELXpI8XZ6fPBzyPERL4Uitx9fzLAIZhjyXyy9Rk+SQgiRQJOkEEIk0CQphBAJsjVJmjJniHy7PTSxvQ+Y6PtIgARNyNAYWJyegmEZL2693nH/4M3kfe8DMXJ0nlevXrnlXGPqCOM3ZZu2ZSgv+x+H4kvQNAnL1RPM5FtordQBW+iHDDe5iTR56yBc0g+/ydQkb2/8dVXyOoTmRHP5GkvdBWEhQtbQPhoP9cKVhmy/Mobu8tybme3x7FKj6d445hUxMLiD9xu18pX93yOa5ATDd6DfRvrXX+KMQIsNCjq6jsEvNMNHihhw8lgIEZuHrqEnSSGESKBJUgghEmiSFEKIBB87dJfhDiv0A3ogLeIHg1QXaF1BEkSCBvrl+++975aPCBLodn581C4+2rzfPoN9cwMuGF4a+EGxHWqx+32o89AX2eJ4M0ghRYPvlgy8wDIDONoiPA67mo3u/e+327yAi2c7ekoRhEydCoHGY0xgpKcXl9n8hLDnko3A8NU1CAPhGsLxbToeG6//Uxe8RAGTJs9rELSCYzVNuEHNbMJNy91hmHUKNuUqSwZJo8mYv+QDTdMs9EHyfcC1BoEx9CQphBAJNEkKIUQCTZJCCJGgWD9OZxwhhPiEoCdJIYRIoElSCCESaJIUQogEmiSFECKBJkkhhEigSVIIIRJklyU+Q3/hFf1tdxsfl3WtD7ZZWArIEq/j2ZctvfdwueTptsV8z9K5jS9xqtguOVJaVyLTjPvE5V//NR/H9prPfvZtt7xDL+mqRn0YMqy2mzCK7Kb1P2Mf9K7xy//sC/85OjYzsz/7p/6IW27YegFlixuUdnWRmLkdylRZatdt/aX3Iz/xU9Gx/a0f/gG3XKHNBktDB/R9nsI6QFsRb8Y4tRHf+dG/8ZPRsZmZ/bUf+j6/LrQpYfzYulyPZRuxjtNpwLIvtfsHP/WF6Nh+4E9/l1s+Hv16enS26Hu/3ofDQ7DOU+/vwRk7sCx+G//jf301OjYzs9/1jc/d8n7r4w1PJ19KfDz6+SBWYsiyRJY+8p597K+XeOpJUgghEmiSFEKIBJokhRAiQbYmyRh6RpMxap0RRVw2CyPB2G5zibW0vDS+NT2+sqSe6L8/DGHrUep8XAejpS5ypdcC2zO0DdqvRmLcmqCVaXr/U2xx7iqsq8N4b6BPU7M0M9tSt2wR5dbmxfy30AvZLWDleTW/3jryHEBNcqnYaiK/UpetlZeCsWZe32QL1tiWeC7DT+WNb5n9WObZ63XzlNbC6ya8huoVOiDu0ZK/T0CtnXoy31nwPom1h2V7Bs5Lm4i+fw09SQohRAJNkkIIkUCTpBBCJMgWrpqd/1++gly4ab33j/a04xR6HI8HxLdDplrZhzTBHi1tC+hkjOSfer+8bX07SzOzDbyVq6F9aaZ2td/7dW+3fr2G9g4NNLJtpO1oW/p1lPB5bppQx7zEFtoTbZt76If7jV/eRq6iXeP3qW3QAiLzytvWaINa4ZjTD7ugvUNspUWdXJ7yuzfYTYV2wBgfbXiBr3AOr6EZP6NWb7ldW3n/lNBDSz+44B5Zw/ar4+x/VuA5a2Wf2gQj9Mzj0c8Ru51vS13hOi0jbSxqavXwOn+cYEg9SQohRAJNkkIIkUCTpBBCJMjWJFtoXCX0jsPJt1tlu8cp4nmkl6+gCS5ST30J1vDW2DV6MumninWvZU31usKDl9m2dbf1ei41Sbb67KC9xGq3W7SlbaDN0cuYYtPiWJX8vf8BfZy7TSiS7fCdDiutM1t73u6hhVOXgn5LzW+IXHfzTL+rX8d5zPfn8goI6v1xn8B6a33EPzxBow78uEXedVdgYwXOQQnf4TL4sfSRlq1sq7yyH2+mh9Ms9EFz+eUHL93yzY2v7b699ctmZhMyJR4eDn50H0OU1JOkEEIk0CQphBAJNEkKIUSCbE0yqMU+Q9ODZ4m5bW0XalCbjfc2LovXE8Yp9GldIsh6hEa32fpthXpiTP/0+kVR0OuX9zeGmmPgi4TOtoEvsq5CwbSBXrpDDfFuk19Du+nok0QteeuXd1uvdd3uQ/3zBuvcIN9yi/NxiW9455lbpia54rwP0KTOkfreYfTnekTZ/rqEdfyXqK74AoOyfYy3jIjh1CBnXJtzJCMzxozrjnXjE+7ZGZog9T2zUPuf4Ok8H7wGmOIY6IX+WHR4jzDhPcL7yJc0MyuZhbrhu5SnPxfqSVIIIRJokhRCiASaJIUQIoEmSSGESJD94oaCJwVchlne3vjidJrLzcKXQRNe1NRV/hzOpkA0j1Mw7yDoxozhQVAvPhIEVVygRUAEN1Xj2PI9TR1ptMUwWvjBLZKDe5EO5vAWL4W2V17c7Hfhi5sXN7geEJByuw+NwDG+8Rveccs0jzMk4YDmccdzeOx6mMUPB/9yZ6nzDcd8gXYaYVTHfTIv19fNoIkeJu9jn/dC84RjcUYTr+OR60nf42ZhgESDa2c+579sndB4rIT5fQqCtP11xkBds/DlE+ct/j4HPUkKIUQCTZJCCJFAk6QQQiTI1iSpbzBcs4budh68/vF4DE2mVIsmaEVTka8fTGh6ZLMfz4Am5ZsNG5mFh4I+3w0M8dtNpiYJvZR/mZqGTbL85zddqPm1EB0r6JZVxIB+cXw4d1sY0XcNAoqhxe524XF4/txr0m/d+uUXt94kfol3P/XCLVMlGyavJ7L/GYNYzMzWg782z/jMuuZfdzOuOzar6mFmPyOFd4gk/DIMpkeDrNMpNFHHODFkhhojCzBwZTaRBnQTNMeq+nhFAmbhe4GVgRu4pVlUwfcQZmYHVAYwRCe7ed8b6ElSCCESaJIUQogEmiSFECJBfsAFtJUOXrmy9XrH6RG+yIjOU1doPl77dTSRUIxLVNBPxsl7sOj5YvhpxMYZhN/yS8WUp2+w2T01xxbLHZa3UU0SIbbwD+aGb5iZldAkedy3CNXd7b0GyUZnZmZ7+GRvoEludnnntsV1FmhKk/99By36GGbamkFPH6kjTvl67oiAhxGhET1CIu6hJ9KzaWY2IdyZ2xgyQ4H7ARo/JMmFvsgZwdWxYGTcKDM0YWOjtgQN3wNgPxfDyUMzwbkPzcBTz3nK+3NvbvbZ43uNniSFECKBJkkhhEigSVIIIRIU68fpjCOEEJ8Q9CQphBAJNEkKIUQCTZJCCJFAk6QQQiTQJCmEEAmyK272O7Qdbf38ut/79I8TklbmIawSaCqk46BKpEPKzq/++suL4/v0p3xFBxOImU5SYc9bVtdEfsbEblaa/Jv/8KXo2P7kH/t2t7xDig7ThXbYb37eLGy3yXa3Hc7PX/87/zQ6NjOzH/uRP++Wn934Y/UCKT/P976K4VO3YfLLO0gBevHMH6vdDlU83/FD0bGd/8tPuOVpYYqOr8p4PPiKlvdfhulT7798cMsv7/x37h/9On/wR/9RdGxmZn/zh/2xuzv4lJz7R7/uD+8f3fKxj7RtNX9sGKZ99+DH/6//7S9Ex/aHP/+tbnlERc2MxBxW3ATJWmbWIwUo1nb2Tb78m48Xf/eNb/vrqgye2Vb8HpVhG3+NmZndP/rtoUtt0JL4q+/fXRzfb29XCCHERTRJCiFEAk2SQgiR4AndEv0/9xP0i/sHpCWjkIef/+gjPrHj+Y1Pq2ansxTshsjUYiYos8sbk2Fi62R3tprC5gVaJI93aG3ItCEm/DSRrpFMHq/wmbrOT7LhupgoxGT1FolL7GBoZsZCLmrEAxKUwhyhjzijoybTnc6D18juHo9YDrv33aE74uPZj+0cSw66ABODhoGJPeyW6K+Ffgw3tq64V5AKtM5598U0It0I6UHFzER2pPCM4T2xIEpowTpqthRNwPX36JbK66xialCkI2iLVP0Z+3RGynsOepIUQogEmiSFECKBJkkhhEiQrUnudj7Rd8H0egf/1wa6W7sJ9Q1qdS26pxVrRHS4ADVH6iubjff2bbZeu1ginjBKbdQ5Y93aYrCz4bXlpmInxIgmWVMvRbp5k3/sGiSTU6OkHk29eY545fre+2SPpdeyzoO/Pl5cGNurV15jZOdAduGkL/GDu4gm+YBOniev1Z2GfD33DP/vGb7H48lrjicInsdTRCPDdV8W8C+mrYlfY4TmNw5I/udphb5YFeE1tNv4eYCdG3edv89S3O79O4iXr1655QE+zgHdBZaTvzbMzErcOwMO1mLqliiEEF9XNEkKIUQCTZJCCJEgW5M08wLGGTWyZUG/mNdabm/C+t7tFhok1lHkS0PWoqNgoBfCB7mgBni7C7UUaoWBf7DN0yRr1FlXWObYqS9yHGahBlvBn8bzkSLicnRLC/x1vUFHi2hkM7TDE2p+eUy++cLY3kNt7RnX1f2jr2N+PMI3eQwH94gWitQkz0/QJAPNERrlAZrjqYfvM9L5cEbHRdom+yFPlJywHq43OM/Q7/o+1HNfvHjhltvWe4dZK51ih86F5xl6La6ZEzohRiRTa6HfoxGmDWwZmYGeJIUQIoEmSSGESKBJUgghEmRrkkdoPU0DHQ3T7bn3HqZonTMEjH70/jVm16WYUAfOGtLj2f9+Xvx4yjLUKnbb536dZdqbeIkK3rESNak1asK3yI9kLbeZmS1+LCtqmld7gvYCnWaF5jUvfvkEL+C5DL1nB/g0G/g6q0itfIzfeO+V39YA3eoIPyZ/fw71uyNqtY+93//TkH/sDlh/P/v9HHGeihIZqmWoSY64lnvomH2fWX8M32MDEW+BBhhomJG8BfoiK9zXD6fL+ZHk1cnrySt06qXEdQbfZ1eGomTd+HttnDDep9wXv4WeJIUQIoEmSSGESKBJUgghEmiSFEKIBNkvbmgsXVcYXuF4bfFSg4LvR99BqCYM0k8J3a0g4tLM3tGwHRjBn+CC/S2uNUF6TVMjALhkALD/fRDGEQmrGHA+xoEBqllD++i7CBI4IUiA4aYFQwIiL2F47hgWwuvnEl/94N4t9zBr9zjPI15WnCMvYWj4Zi+uc6ZZ28zswztvdu+ndDOtx6O/D8bA4G1W4AXLNPmXU+dz3osbvngpcd3xvcyM8960YQO6GS+iDkf/oma2/MTiEeehwn3Ce3az8euO1UtwnroWnJ2DniSFECKBJkkhhEigSVIIIRJka5LUyUYEGGy26ZCG8zkMyKRuyNAD6gkp2GgqKM5nozCERuRYTNmYiOO9PDYEfzKxmAIidzt2HBDESvM4G9qnGNmsCsuDQTsKTLyx81QkP7Gsebrfy3sEQsxsMFYkf386hwfiAE1vRFHDU0IQ7o/+up7ZXAufH9D4q4/o2rVBz0chxwkG+ktQa65KhtBgLEHYRsTojvF3uCdu9y+yxmZmQTHJ+ez3q0NRxYsXPhDj/t4HLpuZnU7UJP3v2TAvBz1JCiFEAk2SQgiRQJOkEEIkyNYk6QkMms2jCL+uGVDrtQszswGN5Y9HFs/nz+HUNxd6/aD7lQjAWJZQGzphPB10zNtbr5FcgroUvXP0+h0P0NmqUPNbF3/sFpyPSDbBRRigUDH8OPgGtapI+uma1l3nzKCBA6TlGTrWNPvtHKDf9efQtzfj2eDMsNk53+tn8INOI7bPkN2F5yk8DgWOFbXFhUmyF5ioNRd+PZReS4RVUH80C72Mm90Nfp+f4z3Aj8qAmwKrOkGzjF3j+70P96bumuttfhM9SQohRAJNkkIIkUCTpBBCJCjW9SlVvkII8clCT5JCCJFAk6QQQiTQJCmEEAk0SQohRAJNkkIIkUCTpBBCJMiPSkP0Gds1dJ1fVbfxZYixsr9pSsfQM4rsgw8u9+H+zGd8j2yWVG02vlzp2XNfTrXtwrJJxqlt0PLh0+++65b/8U/+THRsf+HPfbdbbtGO4Wbnt92iDDFWnVmtfv8WtEMoELP/Y3//C9GxmZn91b/0J9wyWy8UKDtcUBq4Rv7WrvwM+4RjGz/+9346Ora/8he/zy1PqEU7ozf7gBK+NRIzN6I07f7RtyDoce38i5/9r9GxmZl973f/Abf8iHU9HHyc12qMZQvvC7bisDl9Pn7pV78SHdu3/s5v8ttinwr0mq9QSswywI/W4Y93UIYIR+H7j5f7cL/Y+fjFm2f+nlwRB/jq3rfyYNSamdlu50uFWa7MCMVzfz1TUE+SQgiRQJOkEEIkyP53m6k6/HebqUBMGY51x2NSUG7SdwymezBJnZ3XJnSpWzfh34u69t8pS7+8rnnjrUskp6A74oo06xHSBAN1zMxGJNUsSJd5Sqr7eUIKUImEeAxgxr9USyTRZ8G/lfwXfRnz/j7fI9WH/z6NE2WHdLdCM7O+Z+qSv1Z5baQ44d/pCeMrMJ6FBW5rmLTDf3Pbzv8Lud/sssZWQ9ZhIlKNJKwG3UqHSP4TG2POSNmxMr+Ar+14P/nfL5Bo6sp/Pug+YGFXVs4L0xNS51+jJ0khhEigSVIIIRJokhRCiATZIiA1rhoJxXXNdGokUa+hFkCbCHlKivDh4LvWcby3t7duuW28vlFF07XxM2gkj3dht7YYDw9+bFXhNaamYhI4NMkmHBt1TKZVP0GStHHCuSqZ6k79E7+O/K2dqV1hmV0NL/Fw8sd4hObH7oPsTHkeQt2KadWh6Jt/8I4n2GSg89W4zkaMpx8i1jgm198/uOUqJlJHeHz032NSed143X6GvYpWMLNQ668q2sOe0KYTvHp153+A01DDghfrdsBQswLno6ComoGeJIUQIoEmSSGESKBJUgghEuSXJUKLoC+vbf3v6aNc+lCTnOCtLEtoc08Q1vhd+unOPbxnVeeW1yn8ezFhHzZbHK4lomNG6M9+Pw+l16V2W6+1BCOJ6Lnc3+X/QlebobXO0DfpcQ00ycimSngtR2qFkU58Mc7wg07sNgjt+zyiDC2iSY7QATlWanMpWHZYlP6aYHfBB3gyY1tiM8Qz/ICH6nKpnx8MNUh2CPW/P0FfDTqOWqgD8h5dnnDsgg6nmA+oQQblspH5IdRRef6lSQohxNcVTZJCCJFAk6QQQiTI1iRZm836V863lAZYD/zRN6gx+N+3bX4t927no9BGRGYx4mndUNMLtYoRfrqx8Otsdl7XvAj0wvMJ9b3QSTpoR0Xkb1lRwScJ32HxhD9/8xVrG3WmEVrVHKvLb/zxpQZ5GMIYrhjUGMkMVY86Mq9bs1CVYr0349dSMJptRt39vPpa4gf4ebd7f92ahTkDbePvg+c3L7LGRt2aHsKFfkzcI3UVau5UAWfWRkdq5S8R0zzdunHuKmQl0KNpFp7LMHNCmqQQQnxd0SQphBAJNEkKIUQCTZJCCJEg+83ICAGdZvHTyQvSDNTd732og1kovA4wfI8RE/Ul6sKb2csG34Xxm9kZdRMeiqJiPxIv+raZAZ4rXtyMfJlwpviPMNSIab0u0y8oqlhjnAv0Q1rsZjoFQ3eD/bEw+IGhu8dI/5QYjwf/4qPEOeFbpx6hq30fbodhIPQ/nyLfucQZfZoWhKK06A21v/V9XGKe/x4vjmjgjr1kjDHMCAM5oT8NCioqhEF3m/DFJM3yNICXsaCYC1wL2WaxSYvPM8zaLAzdLXGAqzbzZatbhxBCiItokhRCiASaJIUQIkG2JtlAs2vbtFH12nKMoC/wE4yf7EXcVmz8hYCFM/WZUN9oK7/P1EhOmabjvmegAjQxiGITAieaORJ+ivNBna1nsGyCh0dowTB+s7cTzwpDJ8zC3tYMwTgPaZP418b2gF7LOBQFzMvzQO08FP0Y4DHi+K9PcOJzzwuET7fo977CsB0zVJe4Fk9ohjYPmVo9dn3BaFect67DWCMBEgMar/G+Gsd8Iz6vs2sa5cgClkiA7jxRm6dG7N9d5KAnSSGESKBJUgghEmiSFEKIBB+7ERhh2CU1yHPEF0cNgrrnE3pZBQGeDTxfJXyD9BEej2FTrxnj6xp4rDJ9nAwDoSZZIBy1gh9zmiJBA5BjKhz/M/XdBEcEbjAMZIHOwySSWOguvZMjPHvzmqeZDgjCmNEkrQxCEHieIxoUgh8YzLvZ7LLGZmZ2s3+G8SE4ZeF9gOs0osNNON7HEzTibd6zzQQf7wqtu2o4Vvp1w3NE/ZzXNsO4U/Czy8L3HNgW/MTDGuraBTTIEhox54kc9CQphBAJNEkKIUQCTZJCCJGgWD9OCqUQQnxC0JOkEEIk0CQphBAJNEkKIUQCTZJCCJFAk6QQQiTQJCmEEAmyyxI3G1/es6LciiWGLGMcItFdLEPcb3052IoYqfc+RGzWG7zz4ja5fZZNbrY+FqqLxLrzLwjj4xtEWv3cF78YHdt3/v5v9z9Y2SPb72cdpLaF5Y+M9KeTi+0cfvY//kJ0bGZm3/UHf69bHlG6NQ++HGxBWFoVaX3BEbO8cFp8KeAvfOn96Ni+7XNvueUV9ZgsSyzQ+mK7fR6ODcfqcPRjaxCn9Yv//cvRsZmZ/Z7PfdavC/F57H3NeLBYX/AzotFo0mMHiw/vfcuC19xsUFZbsz2D3881CMEL4fh5X6/owf5wvlym+Gzr5xTes0GEIMp1i0hVcMkyTxx/xvr14/UySj1JCiFEAk2SQgiRQJOkEEIkyNYkS0Tar4j7isXQe0K9g9+ZEcXPbaa4ptExImkcqPuE41+gv9zufDvQZpt3+NhaYoFuU7DtZenXO4yh5vQh9FlqrE9pfXE6seXBlfYSbCMa1YagYaPV6jzknVt2d23QorVCmw7qqfevHoJ1Dmg5UKNtgY35x+6AlrcDjw208LLkcQkP3v7GX2cLIs+GzHa8VUnNz4+F7XaL8no4IcfboHV08YRWxlXNeEVEBmI4bCdczGHLFR5N6s+MrstBT5JCCJFAk6QQQiTQJCmEEAmyNcm28B+dV8bze52HrRzrxmtHZjGPmNccui70Ll6C7Rro5yLnk9dj1lOo+9XYh/3e/37J7C8xIWZ+QlvOCjJJ1+JvV0RHWXDszievsZ5OebqVmdnD/dEt77Z+R2e0tA3aAszhsa6pQeNYTRmePDOzCfu+gQbJY0NZjTq3Wdi6uOv8ea6KsF3GJU5Hf9001MbRiuHx9OiWd7uwVUSHa3kp/TqaGhfiJXAZ8Z5doAFO8MPG+qfUtf9hC424zO/eYAVb+fJcQstnC5a5CZ/xOGSeyWrMbMfrxiGEEOIimiSFECKBJkkhhEiQrUlSr6DHqWtDzdF9vgp1Hmp+TQXdM1LXeonHR6/1cHv0qxmW2ZLVzKyH5+7h4NvO1k3o04pxOPnvFfQhwqd4OsAD2YXHlp634+GY/H0S9IQ9n+kh9ePlcZnWUPerZpxv+GqHiI4Zg77DO55nXCIl9NKuDnXtFj/bwCfZT/nteIce+4FjeQPP4wodbuzDFqcH7OMNxPA6LO6PwlpoqsAN3hNMM33L4f3Xdf4erTEPjJGMhkuMg98epff6ihe7t3Bb3Eceg2utsWPoSVIIIRJokhRCiASaJIUQIkF+7TY0O/oSy8ZrUDm5edQNe9RTP0U/CHSzPq2rbeFP67Y+W8/MbIXpjnWg5zHUk2JM0LhO0Dbnya+nQY1qWXhdyyz0D44jvX+oR07A/Xq4u3PLrMfluZws1PBW6E306FW5HljUH5+h4dWwvRXIB1wi8mKBdfZY5zDle+lGeC5HbL+Czr6uzFkNBzjiZ+fCezFvn+VpkltkpB6PXrcecd4rnOfIa4TgM5wX1idIfpwRGDcwoxJ7maHlR9YZum/9T/b7/PviNXqSFEKIBJokhRAigSZJIYRIoElSCCESZL+4YaMviv3XZluK4x+tE8ECFQNJ8wMy+aKGUE8eEDJRTBGVGqJ0zxc153gDpnDjeAG0UNzncfCff/XqVWRs/jtt4188nSKBHZc447NnHEsK6HyRs0Qk9G7rXxpgl23NDRpg86cZjZ1gXt63Xpifp3BsC16kFaXf3z7ynUu0rT/uDBtmKG/YoC68BTcdTOAMM0HhwEXwco+Ns3q8MFwxtt02fEHEAgoGiDzlxQ3rHYLv4sUpp4PY/MB3vfxIbB66hp4khRAigSZJIYRIoElSCCESZGuSbFy+sFD/xCbla3LZLDTSdtBipidoQ/xs26YNtxObjkVCemmU5XfGx7xi/jO0SwYLQMayZUo3gDczqxp/rE6z3waDcVPQ7M7gkXFm4zJoXRHX8czrhU3fDnna0PDg9+tmj2ZsG68JzjiPJwZQWDheGsCfct21NMVDEwsa0GF8deQOvNkguBdhILFQjBg9g5dxD1ICZJMsHhczs65DyATGcrUf4BtUCM0tIUqyR9oKbbyONB2rG39AWWTxMfIt9CQphBApNEkKIUQCTZJCCJEgW5M8wSPIQnf6Dkt4nIo6Mh9DAxlopsvvx2Swz9n52nimdCCpWRjiwUZF15qNvebu7AMtOugmxkBjCDv7fdgsakLgKANL2TQ+xVIhSABHoy0RZkL/6DlsOkYf5A5BsX00niCkRbe1huEUuC5fPT645W4TBpeUWAf10+kpYc8MO+Y1G5r73GIdCUcOrg8sninWXSDQjrHcVbwHECAR0WZPR3+8Z5yfiIx5mRLaK+cDaLF4DWItxXwzaxHqQY10zAyleRM9SQohRAJNkkIIkUCTpBBCJCjWpxRICyHEJww9SQohRAJNkkIIkUCTpBBCJNAkKYQQCTRJCiFEguyKmxYtY5kqzjajTH3ZbMIWomXBqh3vhi9Q2XFMJMe0HdO9/Xfb1rv7mXwTa1/LnzHRhfs4DnGjwO7GbxvFP1ah0uGtG5908+xmH6zzOPkqlyOqXtiW9L0PwqqY13z6Hb+9ickp6HraonpmnsK2qKyMKZk5A1PFlx8fo2P7zO2tHwuuwzOqMg5IWb+5CdvxtkhhYhrVgDaxd8f42MzMbrc4N2xDzPsCx7Yqw+eUXedvy7JcsOy/82uvfEXXaz7zlj92/dknsE8LEomQyjMt4T1xHlBSg4/wOxPLZN7gBu1deb/zuoqlYZHdzq+TVXF9z0q16+YePUkKIUQCTZJCCJFAk6QQQiTI1iSpg9TQpRokl1zrXmhmNjP1BwIHOzSmoObJ8VKT3CAdJqZJUmeljjkn9Bb3PXyOW6LO1kPr/CCi1x0Xppd7vXTThBrwJWokYVMbaqDvBk3tIrJOkJ4N7SeWzhOD6VEju/Ph83Xrr5ljH3aNPI3+2txBV+yavLGZmZUtOkcydgb7vaBlIa8pM7N18ceuqf0R3+7CVKgY7FhJvXR49NsOwuwjemmFwP9pRtdUCu4JWOxXYJn3LO9pXvNm4bzD86FkciGE+DqjSVIIIRJokhRCiAT5oh/g//bUC7gco0LXugJ6xlMCipgiTi2CWgXHt9+HXsTt1nuuOJ7TKdS7YpSVP8zUUmasdza/3iWSRE39iH68Ys7/+zfDf1YgIbpmMnnB8xTxmNZ+gOczfZ15x44p4dQoV+jaTQc/bBFGZY/wQTIBmynvKU6D368Kx6aCn7hcoGFGzm0gDUKbj30nxox7IBgLvc74PJPCzcxKiJJ1eT3N/BLX3jnQ48j5gve8WfgeIXzP8XRRUk+SQgiRQJOkEEIk0CQphBAJsjVJ/v9/rY6ZHsLNJpyP2Z2NGgT9eilCLcJDDZIaWUz/3F3xo22gWV5it/N6Z1CjCu9fvfU+vbYLPY+nBz/+CZprMecLa8PB64MFzt2K4m3WYcfqj+sr+tHa5l16FXSrIzyOPY7d/lm6ltrM7Ixr9zT6YzkHhsHLTIvffgP/Z4Pxs2NhqECa1Tye2Ifj+boH2czs5at7v23opfMVbZNauZlZifGz9vkpnSbHgbXj8AvDa0sPZuy9R9emPb8xb+U19CQphBAJNEkKIUQCTZJCCJEgW5MM6pYX5keiDrO6rn8w85CaSaye+hL0QbLuk3Ddh8MxXCf0iwl6TFnl/Y2ZUIt7+9xnHK4r9Vx/bIdTmAVZoQaYNctdmW+BbUp/rOi5nHCumTdJ/5pZ6IGrgjzSvPHxuuF1R58ktXCWUpuZrSiXXla/jqfUHzOzgL68EceSy1FVEL7TlbmOVfrafk0PeXCBfku9jvsdO0eBrol5IeatvMQ4jcllvrPgHGOx7ARkSlT0Xj9Bb36NniSFECKBJkkhhEigSVIIIRJokhRCiATZ6j4F8RkiatV4gbSBgEqj+EfrTAd25obamoWCchi2yfEhNCAiONNgTtP0OIaBqTG47zuY0EeEAhwPvrFTEOT60Q/dYoMQjabO//tHQ3CBF1Irjen4fRm8vDCbML7HRxig28yXXgXEewbHjvj9oz92DHUwM6vwcqXEiye+MEixjHgxM6bvk5zQlgrnrm19MUHuC8229ffgNOGl0sQigesv14KgXN5XkWvhEmGBCopJsG4axddImAYDNvgSkoUbOehJUgghEmiSFEKIBJokhRAiQbaAUDOgc4D2MkGrMBT+RwIyzdL6AJt7pbgWwEGDdl1AV4usk+oRNcmyyDt8DCzoEdZ7QtgGtU4aYs3MVhxvai3X27j/NgNMvCWODc3k1Oxo6I6NZ4E0uIZSYXxsC5tVpQMUAh07Yh5muHNVcnB5YzOzIKE3FgqRWndMXqTux2v5KUUWb0KNkTo8l6eY5kdzPK5VzhMpuBvXdovafBUJe+Z9vMz8Tu7oLq9TCCHEG2iSFEKIBJokhRAiQbYmyYBLNtQZp7S+keO5ouYQ9QdegOui53IKNFNPLMCTWl3QVKiN6awh6JVkK3UzhtwygIErsFBbeUooQ7gyv+9F0KAN28oQk+j1KxBUMWSGs1IXaxDcsdvCQwjRrz+FAbUTvIwrtHEGBqfoqNXjvNADyyMV05v5k48TFGsW9ya/yXXPZvj7hfo0Myee4G2mnsl7kNorQ3aCg21mBfTl8Fp9+nOhniSFECKBJkkhhEigSVIIIRIUa04xqRBCfELRk6QQQiTQJCmEEAk0SQohRAJNkkIIkUCTpBBCJNAkKYQQCTRJCiFEAk2SQgiRQJOkEEIk+D+h7ZJTcinBZQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the patch encoding layer"
      ],
      "metadata": {
        "id": "ulp8PvpOP0na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.projection_dim = projection_dim\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches + 1, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        batch_size = tf.shape(patch)[0]\n",
        "        cls_token = tf.zeros((batch_size, 1, self.projection_dim))\n",
        "        patch = tf.concat([cls_token, self.projection(patch)], axis=1) # Add learnable classifier token\n",
        "        positions = tf.range(start=0, limit=self.num_patches, dtype=tf.int32)\n",
        "        positions = self.position_embedding(positions) # Add learnable positioning embedding\n",
        "        encoded = patch + positions\n",
        "        return encoded\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"num_patches\": self.num_patches, \"projection_dim\": self.projection_dim})\n",
        "        return config"
      ],
      "metadata": {
        "id": "s8YYRPO8MAYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build ViT model"
      ],
      "metadata": {
        "id": "oCaOVcNeQtud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vit():\n",
        "    inputs = keras.Input(shape=(64, 64, 3))\n",
        "    patches = Patches(patch_size)(inputs)\n",
        "    # Encode patches\n",
        "    encoded_patches = PatchEncoder(num_patches + 1, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Take classification token from output of final transformer layer as representation\n",
        "    representation = encoded_patches[:, 0, :]\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(representation)\n",
        "    # Add MLP\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.1)\n",
        "    # Classify outputs\n",
        "    vec = layers.Dense(37, activation='sigmoid')(features)\n",
        "    # Create model\n",
        "    model = keras.Model(inputs=inputs, outputs=vec)\n",
        "    return model"
      ],
      "metadata": {
        "id": "wd2cP-45OXwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit = build_vit()\n",
        "vit.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nosbWMIqSXLo",
        "outputId": "f1859546-37ec-4731-c91d-572fa03e996a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ patches_1 (\u001b[38;5;33mPatches\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m192\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ patch_encoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m16,576\u001b[0m │ patches_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mPatchEncoder\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ patch_encoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m132,672\u001b[0m │ layer_normalization[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add (\u001b[38;5;33mAdd\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention[\u001b[38;5;34m…\u001b[0m │\n",
              "│                           │                        │                │ patch_encoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │          \u001b[38;5;34m8,320\u001b[0m │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │          \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_1    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m132,672\u001b[0m │ layer_normalization_2… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_2… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │          \u001b[38;5;34m8,320\u001b[0m │ layer_normalization_3… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │          \u001b[38;5;34m8,256\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_3 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_4     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_2    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m132,672\u001b[0m │ layer_normalization_4… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_4… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_4 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_5     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │          \u001b[38;5;34m8,320\u001b[0m │ layer_normalization_5… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │          \u001b[38;5;34m8,256\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_5 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_6     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_3    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m132,672\u001b[0m │ layer_normalization_6… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_6… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_6 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_7     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │          \u001b[38;5;34m8,320\u001b[0m │ layer_normalization_7… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │          \u001b[38;5;34m8,256\u001b[0m │ dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_7 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                           │                        │                │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_8     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_4    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m132,672\u001b[0m │ layer_normalization_8… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_8… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_8 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_9     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │          \u001b[38;5;34m8,320\u001b[0m │ layer_normalization_9… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │          \u001b[38;5;34m8,256\u001b[0m │ dropout_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_9 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dropout_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                           │                        │                │ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_10    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_5    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m132,672\u001b[0m │ layer_normalization_1… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_10 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_11    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │          \u001b[38;5;34m8,320\u001b[0m │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │          \u001b[38;5;34m8,256\u001b[0m │ dropout_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_17 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dense_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_11 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dropout_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                           │                        │                │ add_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_12    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_6    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m132,672\u001b[0m │ layer_normalization_1… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_12 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_13    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │          \u001b[38;5;34m8,320\u001b[0m │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │          \u001b[38;5;34m8,256\u001b[0m │ dropout_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dense_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_13 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dropout_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                           │                        │                │ add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_14    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_7    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m132,672\u001b[0m │ layer_normalization_1… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_14 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_15    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │          \u001b[38;5;34m8,320\u001b[0m │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │          \u001b[38;5;34m8,256\u001b[0m │ dropout_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_23 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dense_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_15 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                           │                        │                │ add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_16    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_8    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m132,672\u001b[0m │ layer_normalization_1… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_16 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_17    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │          \u001b[38;5;34m8,320\u001b[0m │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │          \u001b[38;5;34m8,256\u001b[0m │ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dense_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_17 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dropout_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                           │                        │                │ add_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_18    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_9    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m132,672\u001b[0m │ layer_normalization_1… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_18 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_19    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │          \u001b[38;5;34m8,320\u001b[0m │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_28 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │          \u001b[38;5;34m8,256\u001b[0m │ dropout_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_29 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dense_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_19 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dropout_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                           │                        │                │ add_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_20    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_10   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m132,672\u001b[0m │ layer_normalization_2… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_2… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_20 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_21    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │          \u001b[38;5;34m8,320\u001b[0m │ layer_normalization_2… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_31 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │          \u001b[38;5;34m8,256\u001b[0m │ dropout_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_32 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dense_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_21 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dropout_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                           │                        │                │ add_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_22    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_11   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m132,672\u001b[0m │ layer_normalization_2… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_2… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_22 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_23    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │            \u001b[38;5;34m128\u001b[0m │ add_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │          \u001b[38;5;34m8,320\u001b[0m │ layer_normalization_2… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_34 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │          \u001b[38;5;34m8,256\u001b[0m │ dropout_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_35 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dense_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_23 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dropout_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                           │                        │                │ add_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item (\u001b[38;5;33mGetItem\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ add_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_24    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │            \u001b[38;5;34m128\u001b[0m │ get_item[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_25 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m4,160\u001b[0m │ layer_normalization_2… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_36 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ dense_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m4,160\u001b[0m │ dropout_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_37 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ dense_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m)             │          \u001b[38;5;34m2,405\u001b[0m │ dropout_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ patches_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Patches</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ patch_encoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,576</span> │ patches_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PatchEncoder</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ patch_encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">132,672</span> │ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                           │                        │                │ patch_encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_1    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">132,672</span> │ layer_normalization_2… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_2… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ layer_normalization_3… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_4     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_2    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">132,672</span> │ layer_normalization_4… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_4… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_5     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ layer_normalization_5… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_6     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_3    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">132,672</span> │ layer_normalization_6… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_6… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_7     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ layer_normalization_7… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                           │                        │                │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_8     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_4    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">132,672</span> │ layer_normalization_8… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_8… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_9     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ layer_normalization_9… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                           │                        │                │ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_10    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_5    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">132,672</span> │ layer_normalization_1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_11    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                           │                        │                │ add_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_12    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_6    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">132,672</span> │ layer_normalization_1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_13    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                           │                        │                │ add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_14    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_7    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">132,672</span> │ layer_normalization_1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_15    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                           │                        │                │ add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_16    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_8    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">132,672</span> │ layer_normalization_1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_17    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                           │                        │                │ add_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_18    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_9    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">132,672</span> │ layer_normalization_1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_19    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                           │                        │                │ add_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_20    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_10   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">132,672</span> │ layer_normalization_2… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_2… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_21    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ layer_normalization_2… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                           │                        │                │ add_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_22    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_11   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">132,672</span> │ layer_normalization_2… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_2… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_23    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ layer_normalization_2… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                           │                        │                │ add_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_24    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ get_item[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ layer_normalization_2… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ dropout_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,405</span> │ dropout_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,821,477\u001b[0m (6.95 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,821,477</span> (6.95 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,821,477\u001b[0m (6.95 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,821,477</span> (6.95 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save weights and training history"
      ],
      "metadata": {
        "id": "EqV16IFwlmZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where checkpoints and history will be saved\n",
        "checkpoint_dir = '/content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/'\n",
        "history_file = '/content/drive/MyDrive/GalaxyZoo/vitb16_training_history.csv'\n",
        "\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "# Callback for model checkpoints\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_dir, 'vitb16_epoch_{epoch:02d}_val_loss_{val_loss:.2f}.keras'),\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Callback for saving training history\n",
        "csv_logger = CSVLogger(history_file, append=True)"
      ],
      "metadata": {
        "id": "SZInDfNk7hs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EarlyStopping callback\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=50, verbose=1, mode='min', restore_best_weights=True)"
      ],
      "metadata": {
        "id": "JZVc7WAOamMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "vit.compile(optimizer='adam',\n",
        "            loss = custom_loss,\n",
        "            metrics=[RootMeanSquaredError()])"
      ],
      "metadata": {
        "id": "ks4rV1x0Swqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "gPLgSPRvqKkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = vit.fit(train_generator, validation_data=(X_val, y_val), epochs=300, callbacks=[checkpoint_callback, csv_logger, early_stopping_callback])"
      ],
      "metadata": {
        "id": "Nc2N5S_OVlgV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16b9e7aa-4767-45d4-c598-57194a237445"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489ms/step - loss: 0.0838 - root_mean_squared_error: 0.2751\n",
            "Epoch 1: val_loss improved from inf to 0.02777, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_01_val_loss_0.03.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 542ms/step - loss: 0.0837 - root_mean_squared_error: 0.2749 - val_loss: 0.0278 - val_root_mean_squared_error: 0.1652\n",
            "Epoch 2/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - loss: 0.0302 - root_mean_squared_error: 0.1713\n",
            "Epoch 2: val_loss improved from 0.02777 to 0.02407, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_02_val_loss_0.02.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 173ms/step - loss: 0.0302 - root_mean_squared_error: 0.1712 - val_loss: 0.0241 - val_root_mean_squared_error: 0.1531\n",
            "Epoch 3/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - loss: 0.0260 - root_mean_squared_error: 0.1586\n",
            "Epoch 3: val_loss improved from 0.02407 to 0.02326, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_03_val_loss_0.02.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 173ms/step - loss: 0.0260 - root_mean_squared_error: 0.1586 - val_loss: 0.0233 - val_root_mean_squared_error: 0.1507\n",
            "Epoch 4/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - loss: 0.0251 - root_mean_squared_error: 0.1561\n",
            "Epoch 4: val_loss did not improve from 0.02326\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 173ms/step - loss: 0.0251 - root_mean_squared_error: 0.1561 - val_loss: 0.0235 - val_root_mean_squared_error: 0.1517\n",
            "Epoch 5/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - loss: 0.0242 - root_mean_squared_error: 0.1535\n",
            "Epoch 5: val_loss improved from 0.02326 to 0.02158, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_05_val_loss_0.02.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 174ms/step - loss: 0.0242 - root_mean_squared_error: 0.1535 - val_loss: 0.0216 - val_root_mean_squared_error: 0.1450\n",
            "Epoch 6/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - loss: 0.0233 - root_mean_squared_error: 0.1505\n",
            "Epoch 6: val_loss improved from 0.02158 to 0.02049, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_06_val_loss_0.02.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 174ms/step - loss: 0.0233 - root_mean_squared_error: 0.1505 - val_loss: 0.0205 - val_root_mean_squared_error: 0.1415\n",
            "Epoch 7/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0224 - root_mean_squared_error: 0.1477\n",
            "Epoch 7: val_loss improved from 0.02049 to 0.02020, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_07_val_loss_0.02.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 171ms/step - loss: 0.0224 - root_mean_squared_error: 0.1477 - val_loss: 0.0202 - val_root_mean_squared_error: 0.1405\n",
            "Epoch 8/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0218 - root_mean_squared_error: 0.1455\n",
            "Epoch 8: val_loss improved from 0.02020 to 0.01935, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_08_val_loss_0.02.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 171ms/step - loss: 0.0218 - root_mean_squared_error: 0.1455 - val_loss: 0.0194 - val_root_mean_squared_error: 0.1373\n",
            "Epoch 9/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0212 - root_mean_squared_error: 0.1438\n",
            "Epoch 9: val_loss improved from 0.01935 to 0.01893, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_09_val_loss_0.02.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 172ms/step - loss: 0.0212 - root_mean_squared_error: 0.1438 - val_loss: 0.0189 - val_root_mean_squared_error: 0.1358\n",
            "Epoch 10/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0208 - root_mean_squared_error: 0.1421\n",
            "Epoch 10: val_loss did not improve from 0.01893\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 170ms/step - loss: 0.0207 - root_mean_squared_error: 0.1421 - val_loss: 0.0192 - val_root_mean_squared_error: 0.1370\n",
            "Epoch 11/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0205 - root_mean_squared_error: 0.1413\n",
            "Epoch 11: val_loss improved from 0.01893 to 0.01858, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_11_val_loss_0.02.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 172ms/step - loss: 0.0205 - root_mean_squared_error: 0.1413 - val_loss: 0.0186 - val_root_mean_squared_error: 0.1345\n",
            "Epoch 12/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - loss: 0.0198 - root_mean_squared_error: 0.1389\n",
            "Epoch 12: val_loss improved from 0.01858 to 0.01797, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_12_val_loss_0.02.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 174ms/step - loss: 0.0198 - root_mean_squared_error: 0.1389 - val_loss: 0.0180 - val_root_mean_squared_error: 0.1324\n",
            "Epoch 13/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0193 - root_mean_squared_error: 0.1371\n",
            "Epoch 13: val_loss improved from 0.01797 to 0.01795, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_13_val_loss_0.02.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 172ms/step - loss: 0.0193 - root_mean_squared_error: 0.1371 - val_loss: 0.0179 - val_root_mean_squared_error: 0.1322\n",
            "Epoch 14/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0191 - root_mean_squared_error: 0.1361\n",
            "Epoch 14: val_loss improved from 0.01795 to 0.01720, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_14_val_loss_0.02.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 172ms/step - loss: 0.0191 - root_mean_squared_error: 0.1361 - val_loss: 0.0172 - val_root_mean_squared_error: 0.1293\n",
            "Epoch 15/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 0.0187 - root_mean_squared_error: 0.1347\n",
            "Epoch 15: val_loss improved from 0.01720 to 0.01691, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_15_val_loss_0.02.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 172ms/step - loss: 0.0187 - root_mean_squared_error: 0.1347 - val_loss: 0.0169 - val_root_mean_squared_error: 0.1283\n",
            "Epoch 16/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0185 - root_mean_squared_error: 0.1340\n",
            "Epoch 16: val_loss improved from 0.01691 to 0.01658, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_16_val_loss_0.02.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 172ms/step - loss: 0.0185 - root_mean_squared_error: 0.1340 - val_loss: 0.0166 - val_root_mean_squared_error: 0.1271\n",
            "Epoch 17/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 0.0183 - root_mean_squared_error: 0.1334\n",
            "Epoch 17: val_loss did not improve from 0.01658\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 171ms/step - loss: 0.0183 - root_mean_squared_error: 0.1334 - val_loss: 0.0168 - val_root_mean_squared_error: 0.1279\n",
            "Epoch 18/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0182 - root_mean_squared_error: 0.1329\n",
            "Epoch 18: val_loss did not improve from 0.01658\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 170ms/step - loss: 0.0182 - root_mean_squared_error: 0.1329 - val_loss: 0.0170 - val_root_mean_squared_error: 0.1288\n",
            "Epoch 19/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0177 - root_mean_squared_error: 0.1312\n",
            "Epoch 19: val_loss improved from 0.01658 to 0.01608, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_19_val_loss_0.02.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 171ms/step - loss: 0.0177 - root_mean_squared_error: 0.1312 - val_loss: 0.0161 - val_root_mean_squared_error: 0.1250\n",
            "Epoch 20/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0174 - root_mean_squared_error: 0.1301\n",
            "Epoch 20: val_loss did not improve from 0.01608\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0174 - root_mean_squared_error: 0.1301 - val_loss: 0.0162 - val_root_mean_squared_error: 0.1254\n",
            "Epoch 21/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0174 - root_mean_squared_error: 0.1301\n",
            "Epoch 21: val_loss improved from 0.01608 to 0.01580, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_21_val_loss_0.02.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 171ms/step - loss: 0.0174 - root_mean_squared_error: 0.1301 - val_loss: 0.0158 - val_root_mean_squared_error: 0.1240\n",
            "Epoch 22/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0175 - root_mean_squared_error: 0.1303\n",
            "Epoch 22: val_loss did not improve from 0.01580\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0175 - root_mean_squared_error: 0.1303 - val_loss: 0.0162 - val_root_mean_squared_error: 0.1255\n",
            "Epoch 23/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0172 - root_mean_squared_error: 0.1292\n",
            "Epoch 23: val_loss improved from 0.01580 to 0.01550, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_23_val_loss_0.02.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 171ms/step - loss: 0.0172 - root_mean_squared_error: 0.1292 - val_loss: 0.0155 - val_root_mean_squared_error: 0.1228\n",
            "Epoch 24/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0170 - root_mean_squared_error: 0.1283\n",
            "Epoch 24: val_loss did not improve from 0.01550\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 170ms/step - loss: 0.0170 - root_mean_squared_error: 0.1283 - val_loss: 0.0159 - val_root_mean_squared_error: 0.1246\n",
            "Epoch 25/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0168 - root_mean_squared_error: 0.1278\n",
            "Epoch 25: val_loss improved from 0.01550 to 0.01477, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_25_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 171ms/step - loss: 0.0168 - root_mean_squared_error: 0.1278 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1198\n",
            "Epoch 26/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 0.0166 - root_mean_squared_error: 0.1270\n",
            "Epoch 26: val_loss did not improve from 0.01477\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 171ms/step - loss: 0.0166 - root_mean_squared_error: 0.1270 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1199\n",
            "Epoch 27/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - loss: 0.0165 - root_mean_squared_error: 0.1266\n",
            "Epoch 27: val_loss improved from 0.01477 to 0.01469, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_27_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 183ms/step - loss: 0.0165 - root_mean_squared_error: 0.1266 - val_loss: 0.0147 - val_root_mean_squared_error: 0.1195\n",
            "Epoch 28/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0163 - root_mean_squared_error: 0.1256\n",
            "Epoch 28: val_loss improved from 0.01469 to 0.01433, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_28_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 173ms/step - loss: 0.0163 - root_mean_squared_error: 0.1256 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1180\n",
            "Epoch 29/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0160 - root_mean_squared_error: 0.1246\n",
            "Epoch 29: val_loss did not improve from 0.01433\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 170ms/step - loss: 0.0160 - root_mean_squared_error: 0.1246 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1181\n",
            "Epoch 30/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0159 - root_mean_squared_error: 0.1243\n",
            "Epoch 30: val_loss improved from 0.01433 to 0.01404, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_30_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 171ms/step - loss: 0.0159 - root_mean_squared_error: 0.1243 - val_loss: 0.0140 - val_root_mean_squared_error: 0.1168\n",
            "Epoch 31/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 0.0159 - root_mean_squared_error: 0.1242\n",
            "Epoch 31: val_loss did not improve from 0.01404\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 170ms/step - loss: 0.0159 - root_mean_squared_error: 0.1242 - val_loss: 0.0141 - val_root_mean_squared_error: 0.1172\n",
            "Epoch 32/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0157 - root_mean_squared_error: 0.1234\n",
            "Epoch 32: val_loss did not improve from 0.01404\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 170ms/step - loss: 0.0157 - root_mean_squared_error: 0.1234 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1177\n",
            "Epoch 33/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 0.0156 - root_mean_squared_error: 0.1231\n",
            "Epoch 33: val_loss did not improve from 0.01404\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 171ms/step - loss: 0.0156 - root_mean_squared_error: 0.1231 - val_loss: 0.0141 - val_root_mean_squared_error: 0.1170\n",
            "Epoch 34/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0155 - root_mean_squared_error: 0.1225\n",
            "Epoch 34: val_loss improved from 0.01404 to 0.01387, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_34_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 172ms/step - loss: 0.0155 - root_mean_squared_error: 0.1225 - val_loss: 0.0139 - val_root_mean_squared_error: 0.1162\n",
            "Epoch 35/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0154 - root_mean_squared_error: 0.1223\n",
            "Epoch 35: val_loss improved from 0.01387 to 0.01347, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_35_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 171ms/step - loss: 0.0154 - root_mean_squared_error: 0.1223 - val_loss: 0.0135 - val_root_mean_squared_error: 0.1144\n",
            "Epoch 36/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0153 - root_mean_squared_error: 0.1217\n",
            "Epoch 36: val_loss did not improve from 0.01347\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 168ms/step - loss: 0.0153 - root_mean_squared_error: 0.1217 - val_loss: 0.0136 - val_root_mean_squared_error: 0.1148\n",
            "Epoch 37/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0153 - root_mean_squared_error: 0.1220\n",
            "Epoch 37: val_loss did not improve from 0.01347\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 168ms/step - loss: 0.0153 - root_mean_squared_error: 0.1220 - val_loss: 0.0137 - val_root_mean_squared_error: 0.1155\n",
            "Epoch 38/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0151 - root_mean_squared_error: 0.1210\n",
            "Epoch 38: val_loss improved from 0.01347 to 0.01338, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_38_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 171ms/step - loss: 0.0151 - root_mean_squared_error: 0.1210 - val_loss: 0.0134 - val_root_mean_squared_error: 0.1139\n",
            "Epoch 39/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0151 - root_mean_squared_error: 0.1208\n",
            "Epoch 39: val_loss did not improve from 0.01338\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0151 - root_mean_squared_error: 0.1208 - val_loss: 0.0135 - val_root_mean_squared_error: 0.1147\n",
            "Epoch 40/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0151 - root_mean_squared_error: 0.1211\n",
            "Epoch 40: val_loss improved from 0.01338 to 0.01332, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_40_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 172ms/step - loss: 0.0151 - root_mean_squared_error: 0.1211 - val_loss: 0.0133 - val_root_mean_squared_error: 0.1137\n",
            "Epoch 41/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0149 - root_mean_squared_error: 0.1201\n",
            "Epoch 41: val_loss did not improve from 0.01332\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0149 - root_mean_squared_error: 0.1201 - val_loss: 0.0137 - val_root_mean_squared_error: 0.1155\n",
            "Epoch 42/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0149 - root_mean_squared_error: 0.1203\n",
            "Epoch 42: val_loss did not improve from 0.01332\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0149 - root_mean_squared_error: 0.1203 - val_loss: 0.0134 - val_root_mean_squared_error: 0.1143\n",
            "Epoch 43/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0147 - root_mean_squared_error: 0.1193\n",
            "Epoch 43: val_loss did not improve from 0.01332\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0147 - root_mean_squared_error: 0.1193 - val_loss: 0.0133 - val_root_mean_squared_error: 0.1138\n",
            "Epoch 44/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 0.0148 - root_mean_squared_error: 0.1197\n",
            "Epoch 44: val_loss improved from 0.01332 to 0.01297, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_44_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 173ms/step - loss: 0.0148 - root_mean_squared_error: 0.1197 - val_loss: 0.0130 - val_root_mean_squared_error: 0.1121\n",
            "Epoch 45/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0147 - root_mean_squared_error: 0.1195\n",
            "Epoch 45: val_loss improved from 0.01297 to 0.01283, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_45_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 171ms/step - loss: 0.0147 - root_mean_squared_error: 0.1195 - val_loss: 0.0128 - val_root_mean_squared_error: 0.1115\n",
            "Epoch 46/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0145 - root_mean_squared_error: 0.1186\n",
            "Epoch 46: val_loss improved from 0.01283 to 0.01275, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_46_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 172ms/step - loss: 0.0145 - root_mean_squared_error: 0.1186 - val_loss: 0.0127 - val_root_mean_squared_error: 0.1112\n",
            "Epoch 47/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0147 - root_mean_squared_error: 0.1191\n",
            "Epoch 47: val_loss improved from 0.01275 to 0.01265, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_47_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 170ms/step - loss: 0.0147 - root_mean_squared_error: 0.1191 - val_loss: 0.0127 - val_root_mean_squared_error: 0.1107\n",
            "Epoch 48/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0145 - root_mean_squared_error: 0.1185\n",
            "Epoch 48: val_loss did not improve from 0.01265\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 168ms/step - loss: 0.0145 - root_mean_squared_error: 0.1185 - val_loss: 0.0128 - val_root_mean_squared_error: 0.1117\n",
            "Epoch 49/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0145 - root_mean_squared_error: 0.1183\n",
            "Epoch 49: val_loss did not improve from 0.01265\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0145 - root_mean_squared_error: 0.1183 - val_loss: 0.0127 - val_root_mean_squared_error: 0.1111\n",
            "Epoch 50/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.0144 - root_mean_squared_error: 0.1181\n",
            "Epoch 50: val_loss improved from 0.01265 to 0.01261, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_50_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0144 - root_mean_squared_error: 0.1181 - val_loss: 0.0126 - val_root_mean_squared_error: 0.1104\n",
            "Epoch 51/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0143 - root_mean_squared_error: 0.1177\n",
            "Epoch 51: val_loss did not improve from 0.01261\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0143 - root_mean_squared_error: 0.1177 - val_loss: 0.0127 - val_root_mean_squared_error: 0.1107\n",
            "Epoch 52/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0145 - root_mean_squared_error: 0.1184\n",
            "Epoch 52: val_loss improved from 0.01261 to 0.01249, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_52_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 171ms/step - loss: 0.0145 - root_mean_squared_error: 0.1184 - val_loss: 0.0125 - val_root_mean_squared_error: 0.1099\n",
            "Epoch 53/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0144 - root_mean_squared_error: 0.1181\n",
            "Epoch 53: val_loss did not improve from 0.01249\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0144 - root_mean_squared_error: 0.1181 - val_loss: 0.0127 - val_root_mean_squared_error: 0.1108\n",
            "Epoch 54/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - loss: 0.0142 - root_mean_squared_error: 0.1172\n",
            "Epoch 54: val_loss did not improve from 0.01249\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 179ms/step - loss: 0.0142 - root_mean_squared_error: 0.1172 - val_loss: 0.0125 - val_root_mean_squared_error: 0.1102\n",
            "Epoch 55/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0141 - root_mean_squared_error: 0.1170\n",
            "Epoch 55: val_loss improved from 0.01249 to 0.01242, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_55_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 171ms/step - loss: 0.0141 - root_mean_squared_error: 0.1170 - val_loss: 0.0124 - val_root_mean_squared_error: 0.1096\n",
            "Epoch 56/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0141 - root_mean_squared_error: 0.1170\n",
            "Epoch 56: val_loss did not improve from 0.01242\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0141 - root_mean_squared_error: 0.1170 - val_loss: 0.0127 - val_root_mean_squared_error: 0.1110\n",
            "Epoch 57/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0139 - root_mean_squared_error: 0.1160\n",
            "Epoch 57: val_loss did not improve from 0.01242\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0139 - root_mean_squared_error: 0.1160 - val_loss: 0.0125 - val_root_mean_squared_error: 0.1101\n",
            "Epoch 58/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0140 - root_mean_squared_error: 0.1165\n",
            "Epoch 58: val_loss improved from 0.01242 to 0.01229, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_58_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 171ms/step - loss: 0.0140 - root_mean_squared_error: 0.1165 - val_loss: 0.0123 - val_root_mean_squared_error: 0.1091\n",
            "Epoch 59/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0140 - root_mean_squared_error: 0.1165\n",
            "Epoch 59: val_loss improved from 0.01229 to 0.01227, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_59_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 168ms/step - loss: 0.0140 - root_mean_squared_error: 0.1165 - val_loss: 0.0123 - val_root_mean_squared_error: 0.1089\n",
            "Epoch 60/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0140 - root_mean_squared_error: 0.1165\n",
            "Epoch 60: val_loss improved from 0.01227 to 0.01223, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_60_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 168ms/step - loss: 0.0140 - root_mean_squared_error: 0.1165 - val_loss: 0.0122 - val_root_mean_squared_error: 0.1088\n",
            "Epoch 61/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0140 - root_mean_squared_error: 0.1162\n",
            "Epoch 61: val_loss improved from 0.01223 to 0.01220, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_61_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 167ms/step - loss: 0.0140 - root_mean_squared_error: 0.1162 - val_loss: 0.0122 - val_root_mean_squared_error: 0.1086\n",
            "Epoch 62/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.0139 - root_mean_squared_error: 0.1158\n",
            "Epoch 62: val_loss did not improve from 0.01220\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 167ms/step - loss: 0.0139 - root_mean_squared_error: 0.1158 - val_loss: 0.0125 - val_root_mean_squared_error: 0.1100\n",
            "Epoch 63/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0139 - root_mean_squared_error: 0.1158\n",
            "Epoch 63: val_loss improved from 0.01220 to 0.01218, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_63_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 168ms/step - loss: 0.0139 - root_mean_squared_error: 0.1158 - val_loss: 0.0122 - val_root_mean_squared_error: 0.1085\n",
            "Epoch 64/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 0.0138 - root_mean_squared_error: 0.1153\n",
            "Epoch 64: val_loss improved from 0.01218 to 0.01211, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_64_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 167ms/step - loss: 0.0138 - root_mean_squared_error: 0.1153 - val_loss: 0.0121 - val_root_mean_squared_error: 0.1083\n",
            "Epoch 65/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0138 - root_mean_squared_error: 0.1156\n",
            "Epoch 65: val_loss improved from 0.01211 to 0.01209, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_65_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 167ms/step - loss: 0.0138 - root_mean_squared_error: 0.1156 - val_loss: 0.0121 - val_root_mean_squared_error: 0.1081\n",
            "Epoch 66/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0137 - root_mean_squared_error: 0.1152\n",
            "Epoch 66: val_loss improved from 0.01209 to 0.01202, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_66_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 168ms/step - loss: 0.0137 - root_mean_squared_error: 0.1152 - val_loss: 0.0120 - val_root_mean_squared_error: 0.1078\n",
            "Epoch 67/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0136 - root_mean_squared_error: 0.1147\n",
            "Epoch 67: val_loss did not improve from 0.01202\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 167ms/step - loss: 0.0136 - root_mean_squared_error: 0.1147 - val_loss: 0.0121 - val_root_mean_squared_error: 0.1082\n",
            "Epoch 68/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 0.0137 - root_mean_squared_error: 0.1150\n",
            "Epoch 68: val_loss did not improve from 0.01202\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 165ms/step - loss: 0.0137 - root_mean_squared_error: 0.1150 - val_loss: 0.0121 - val_root_mean_squared_error: 0.1080\n",
            "Epoch 69/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0136 - root_mean_squared_error: 0.1146\n",
            "Epoch 69: val_loss improved from 0.01202 to 0.01192, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_69_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0136 - root_mean_squared_error: 0.1146 - val_loss: 0.0119 - val_root_mean_squared_error: 0.1075\n",
            "Epoch 70/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0136 - root_mean_squared_error: 0.1147\n",
            "Epoch 70: val_loss did not improve from 0.01192\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 165ms/step - loss: 0.0136 - root_mean_squared_error: 0.1147 - val_loss: 0.0130 - val_root_mean_squared_error: 0.1125\n",
            "Epoch 71/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0138 - root_mean_squared_error: 0.1153\n",
            "Epoch 71: val_loss did not improve from 0.01192\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 166ms/step - loss: 0.0137 - root_mean_squared_error: 0.1153 - val_loss: 0.0121 - val_root_mean_squared_error: 0.1080\n",
            "Epoch 72/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0135 - root_mean_squared_error: 0.1141\n",
            "Epoch 72: val_loss improved from 0.01192 to 0.01179, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_72_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 168ms/step - loss: 0.0135 - root_mean_squared_error: 0.1141 - val_loss: 0.0118 - val_root_mean_squared_error: 0.1067\n",
            "Epoch 73/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0136 - root_mean_squared_error: 0.1145\n",
            "Epoch 73: val_loss did not improve from 0.01179\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 166ms/step - loss: 0.0136 - root_mean_squared_error: 0.1145 - val_loss: 0.0119 - val_root_mean_squared_error: 0.1073\n",
            "Epoch 74/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0135 - root_mean_squared_error: 0.1141\n",
            "Epoch 74: val_loss did not improve from 0.01179\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 166ms/step - loss: 0.0135 - root_mean_squared_error: 0.1141 - val_loss: 0.0119 - val_root_mean_squared_error: 0.1073\n",
            "Epoch 75/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0135 - root_mean_squared_error: 0.1141\n",
            "Epoch 75: val_loss did not improve from 0.01179\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 166ms/step - loss: 0.0135 - root_mean_squared_error: 0.1141 - val_loss: 0.0120 - val_root_mean_squared_error: 0.1077\n",
            "Epoch 76/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 0.0136 - root_mean_squared_error: 0.1144\n",
            "Epoch 76: val_loss did not improve from 0.01179\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 165ms/step - loss: 0.0136 - root_mean_squared_error: 0.1144 - val_loss: 0.0120 - val_root_mean_squared_error: 0.1075\n",
            "Epoch 77/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0134 - root_mean_squared_error: 0.1136\n",
            "Epoch 77: val_loss improved from 0.01179 to 0.01170, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_77_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 167ms/step - loss: 0.0134 - root_mean_squared_error: 0.1136 - val_loss: 0.0117 - val_root_mean_squared_error: 0.1062\n",
            "Epoch 78/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 0.0135 - root_mean_squared_error: 0.1141\n",
            "Epoch 78: val_loss did not improve from 0.01170\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 165ms/step - loss: 0.0135 - root_mean_squared_error: 0.1141 - val_loss: 0.0118 - val_root_mean_squared_error: 0.1069\n",
            "Epoch 79/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0135 - root_mean_squared_error: 0.1140\n",
            "Epoch 79: val_loss did not improve from 0.01170\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 165ms/step - loss: 0.0135 - root_mean_squared_error: 0.1140 - val_loss: 0.0118 - val_root_mean_squared_error: 0.1066\n",
            "Epoch 80/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 0.0134 - root_mean_squared_error: 0.1139\n",
            "Epoch 80: val_loss did not improve from 0.01170\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 165ms/step - loss: 0.0134 - root_mean_squared_error: 0.1139 - val_loss: 0.0117 - val_root_mean_squared_error: 0.1063\n",
            "Epoch 81/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - loss: 0.0133 - root_mean_squared_error: 0.1133\n",
            "Epoch 81: val_loss did not improve from 0.01170\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 176ms/step - loss: 0.0133 - root_mean_squared_error: 0.1133 - val_loss: 0.0118 - val_root_mean_squared_error: 0.1068\n",
            "Epoch 82/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0133 - root_mean_squared_error: 0.1134\n",
            "Epoch 82: val_loss improved from 0.01170 to 0.01154, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_82_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 167ms/step - loss: 0.0133 - root_mean_squared_error: 0.1134 - val_loss: 0.0115 - val_root_mean_squared_error: 0.1055\n",
            "Epoch 83/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 0.0134 - root_mean_squared_error: 0.1137\n",
            "Epoch 83: val_loss did not improve from 0.01154\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 164ms/step - loss: 0.0134 - root_mean_squared_error: 0.1137 - val_loss: 0.0118 - val_root_mean_squared_error: 0.1067\n",
            "Epoch 84/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0134 - root_mean_squared_error: 0.1136\n",
            "Epoch 84: val_loss did not improve from 0.01154\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 165ms/step - loss: 0.0134 - root_mean_squared_error: 0.1136 - val_loss: 0.0117 - val_root_mean_squared_error: 0.1064\n",
            "Epoch 85/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 0.0132 - root_mean_squared_error: 0.1129\n",
            "Epoch 85: val_loss did not improve from 0.01154\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 165ms/step - loss: 0.0132 - root_mean_squared_error: 0.1129 - val_loss: 0.0116 - val_root_mean_squared_error: 0.1056\n",
            "Epoch 86/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 0.0133 - root_mean_squared_error: 0.1134\n",
            "Epoch 86: val_loss did not improve from 0.01154\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 164ms/step - loss: 0.0133 - root_mean_squared_error: 0.1134 - val_loss: 0.0116 - val_root_mean_squared_error: 0.1056\n",
            "Epoch 87/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0132 - root_mean_squared_error: 0.1127\n",
            "Epoch 87: val_loss did not improve from 0.01154\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 166ms/step - loss: 0.0132 - root_mean_squared_error: 0.1127 - val_loss: 0.0116 - val_root_mean_squared_error: 0.1058\n",
            "Epoch 88/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - loss: 0.0133 - root_mean_squared_error: 0.1131\n",
            "Epoch 88: val_loss did not improve from 0.01154\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 186ms/step - loss: 0.0133 - root_mean_squared_error: 0.1131 - val_loss: 0.0119 - val_root_mean_squared_error: 0.1072\n",
            "Epoch 89/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0132 - root_mean_squared_error: 0.1129\n",
            "Epoch 89: val_loss did not improve from 0.01154\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 167ms/step - loss: 0.0132 - root_mean_squared_error: 0.1129 - val_loss: 0.0116 - val_root_mean_squared_error: 0.1059\n",
            "Epoch 90/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0133 - root_mean_squared_error: 0.1133\n",
            "Epoch 90: val_loss improved from 0.01154 to 0.01141, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_90_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 168ms/step - loss: 0.0133 - root_mean_squared_error: 0.1133 - val_loss: 0.0114 - val_root_mean_squared_error: 0.1048\n",
            "Epoch 91/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0132 - root_mean_squared_error: 0.1126\n",
            "Epoch 91: val_loss did not improve from 0.01141\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 166ms/step - loss: 0.0132 - root_mean_squared_error: 0.1126 - val_loss: 0.0114 - val_root_mean_squared_error: 0.1050\n",
            "Epoch 92/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 0.0131 - root_mean_squared_error: 0.1122\n",
            "Epoch 92: val_loss did not improve from 0.01141\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 164ms/step - loss: 0.0131 - root_mean_squared_error: 0.1122 - val_loss: 0.0116 - val_root_mean_squared_error: 0.1056\n",
            "Epoch 93/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0131 - root_mean_squared_error: 0.1124\n",
            "Epoch 93: val_loss did not improve from 0.01141\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 167ms/step - loss: 0.0131 - root_mean_squared_error: 0.1124 - val_loss: 0.0115 - val_root_mean_squared_error: 0.1054\n",
            "Epoch 94/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.0131 - root_mean_squared_error: 0.1123\n",
            "Epoch 94: val_loss did not improve from 0.01141\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 168ms/step - loss: 0.0131 - root_mean_squared_error: 0.1123 - val_loss: 0.0117 - val_root_mean_squared_error: 0.1065\n",
            "Epoch 95/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0131 - root_mean_squared_error: 0.1123\n",
            "Epoch 95: val_loss did not improve from 0.01141\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 167ms/step - loss: 0.0131 - root_mean_squared_error: 0.1123 - val_loss: 0.0115 - val_root_mean_squared_error: 0.1054\n",
            "Epoch 96/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0131 - root_mean_squared_error: 0.1122\n",
            "Epoch 96: val_loss did not improve from 0.01141\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 167ms/step - loss: 0.0131 - root_mean_squared_error: 0.1122 - val_loss: 0.0114 - val_root_mean_squared_error: 0.1051\n",
            "Epoch 97/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0130 - root_mean_squared_error: 0.1121\n",
            "Epoch 97: val_loss did not improve from 0.01141\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 167ms/step - loss: 0.0130 - root_mean_squared_error: 0.1121 - val_loss: 0.0115 - val_root_mean_squared_error: 0.1055\n",
            "Epoch 98/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0131 - root_mean_squared_error: 0.1124\n",
            "Epoch 98: val_loss did not improve from 0.01141\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 166ms/step - loss: 0.0131 - root_mean_squared_error: 0.1124 - val_loss: 0.0115 - val_root_mean_squared_error: 0.1054\n",
            "Epoch 99/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.0131 - root_mean_squared_error: 0.1122\n",
            "Epoch 99: val_loss improved from 0.01141 to 0.01129, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_99_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 170ms/step - loss: 0.0131 - root_mean_squared_error: 0.1122 - val_loss: 0.0113 - val_root_mean_squared_error: 0.1043\n",
            "Epoch 100/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.0130 - root_mean_squared_error: 0.1120\n",
            "Epoch 100: val_loss did not improve from 0.01129\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 167ms/step - loss: 0.0130 - root_mean_squared_error: 0.1120 - val_loss: 0.0113 - val_root_mean_squared_error: 0.1044\n",
            "Epoch 101/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.0130 - root_mean_squared_error: 0.1118\n",
            "Epoch 101: val_loss did not improve from 0.01129\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 167ms/step - loss: 0.0130 - root_mean_squared_error: 0.1118 - val_loss: 0.0114 - val_root_mean_squared_error: 0.1050\n",
            "Epoch 102/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0129 - root_mean_squared_error: 0.1115\n",
            "Epoch 102: val_loss did not improve from 0.01129\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 168ms/step - loss: 0.0129 - root_mean_squared_error: 0.1115 - val_loss: 0.0114 - val_root_mean_squared_error: 0.1050\n",
            "Epoch 103/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.0130 - root_mean_squared_error: 0.1120\n",
            "Epoch 103: val_loss improved from 0.01129 to 0.01118, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_103_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 170ms/step - loss: 0.0130 - root_mean_squared_error: 0.1120 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1037\n",
            "Epoch 104/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0130 - root_mean_squared_error: 0.1118\n",
            "Epoch 104: val_loss did not improve from 0.01118\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 168ms/step - loss: 0.0130 - root_mean_squared_error: 0.1118 - val_loss: 0.0113 - val_root_mean_squared_error: 0.1046\n",
            "Epoch 105/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0129 - root_mean_squared_error: 0.1117\n",
            "Epoch 105: val_loss did not improve from 0.01118\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0129 - root_mean_squared_error: 0.1117 - val_loss: 0.0115 - val_root_mean_squared_error: 0.1053\n",
            "Epoch 106/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0128 - root_mean_squared_error: 0.1111\n",
            "Epoch 106: val_loss improved from 0.01118 to 0.01112, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_106_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 172ms/step - loss: 0.0128 - root_mean_squared_error: 0.1111 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1035\n",
            "Epoch 107/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0129 - root_mean_squared_error: 0.1116\n",
            "Epoch 107: val_loss did not improve from 0.01112\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0129 - root_mean_squared_error: 0.1116 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1040\n",
            "Epoch 108/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - loss: 0.0130 - root_mean_squared_error: 0.1118\n",
            "Epoch 108: val_loss did not improve from 0.01112\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 181ms/step - loss: 0.0130 - root_mean_squared_error: 0.1118 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1042\n",
            "Epoch 109/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0128 - root_mean_squared_error: 0.1112\n",
            "Epoch 109: val_loss did not improve from 0.01112\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0128 - root_mean_squared_error: 0.1112 - val_loss: 0.0116 - val_root_mean_squared_error: 0.1056\n",
            "Epoch 110/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.0128 - root_mean_squared_error: 0.1110\n",
            "Epoch 110: val_loss did not improve from 0.01112\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 168ms/step - loss: 0.0128 - root_mean_squared_error: 0.1110 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1039\n",
            "Epoch 111/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0129 - root_mean_squared_error: 0.1115\n",
            "Epoch 111: val_loss did not improve from 0.01112\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0129 - root_mean_squared_error: 0.1115 - val_loss: 0.0113 - val_root_mean_squared_error: 0.1043\n",
            "Epoch 112/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0129 - root_mean_squared_error: 0.1116\n",
            "Epoch 112: val_loss did not improve from 0.01112\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 166ms/step - loss: 0.0129 - root_mean_squared_error: 0.1116 - val_loss: 0.0113 - val_root_mean_squared_error: 0.1043\n",
            "Epoch 113/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.0128 - root_mean_squared_error: 0.1110\n",
            "Epoch 113: val_loss did not improve from 0.01112\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 167ms/step - loss: 0.0128 - root_mean_squared_error: 0.1110 - val_loss: 0.0113 - val_root_mean_squared_error: 0.1043\n",
            "Epoch 114/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0127 - root_mean_squared_error: 0.1107\n",
            "Epoch 114: val_loss did not improve from 0.01112\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 166ms/step - loss: 0.0127 - root_mean_squared_error: 0.1107 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1037\n",
            "Epoch 115/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - loss: 0.0127 - root_mean_squared_error: 0.1106\n",
            "Epoch 115: val_loss did not improve from 0.01112\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 187ms/step - loss: 0.0127 - root_mean_squared_error: 0.1106 - val_loss: 0.0115 - val_root_mean_squared_error: 0.1056\n",
            "Epoch 116/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0128 - root_mean_squared_error: 0.1108\n",
            "Epoch 116: val_loss did not improve from 0.01112\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 166ms/step - loss: 0.0128 - root_mean_squared_error: 0.1108 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1039\n",
            "Epoch 117/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 0.0128 - root_mean_squared_error: 0.1110\n",
            "Epoch 117: val_loss did not improve from 0.01112\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 165ms/step - loss: 0.0128 - root_mean_squared_error: 0.1110 - val_loss: 0.0113 - val_root_mean_squared_error: 0.1044\n",
            "Epoch 118/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0128 - root_mean_squared_error: 0.1112\n",
            "Epoch 118: val_loss did not improve from 0.01112\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 165ms/step - loss: 0.0128 - root_mean_squared_error: 0.1112 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1036\n",
            "Epoch 119/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0128 - root_mean_squared_error: 0.1109\n",
            "Epoch 119: val_loss improved from 0.01112 to 0.01104, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_119_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 167ms/step - loss: 0.0128 - root_mean_squared_error: 0.1109 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1031\n",
            "Epoch 120/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0127 - root_mean_squared_error: 0.1104\n",
            "Epoch 120: val_loss improved from 0.01104 to 0.01103, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_120_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 168ms/step - loss: 0.0127 - root_mean_squared_error: 0.1104 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1030\n",
            "Epoch 121/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0127 - root_mean_squared_error: 0.1106\n",
            "Epoch 121: val_loss did not improve from 0.01103\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 166ms/step - loss: 0.0127 - root_mean_squared_error: 0.1106 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1041\n",
            "Epoch 122/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0128 - root_mean_squared_error: 0.1111\n",
            "Epoch 122: val_loss did not improve from 0.01103\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 167ms/step - loss: 0.0128 - root_mean_squared_error: 0.1111 - val_loss: 0.0113 - val_root_mean_squared_error: 0.1041\n",
            "Epoch 123/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0127 - root_mean_squared_error: 0.1104\n",
            "Epoch 123: val_loss did not improve from 0.01103\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 165ms/step - loss: 0.0127 - root_mean_squared_error: 0.1104 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1035\n",
            "Epoch 124/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0127 - root_mean_squared_error: 0.1103\n",
            "Epoch 124: val_loss did not improve from 0.01103\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 166ms/step - loss: 0.0127 - root_mean_squared_error: 0.1103 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1037\n",
            "Epoch 125/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0127 - root_mean_squared_error: 0.1103\n",
            "Epoch 125: val_loss did not improve from 0.01103\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 166ms/step - loss: 0.0127 - root_mean_squared_error: 0.1103 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1033\n",
            "Epoch 126/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0127 - root_mean_squared_error: 0.1107\n",
            "Epoch 126: val_loss did not improve from 0.01103\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 166ms/step - loss: 0.0127 - root_mean_squared_error: 0.1107 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1032\n",
            "Epoch 127/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0126 - root_mean_squared_error: 0.1102\n",
            "Epoch 127: val_loss did not improve from 0.01103\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0126 - root_mean_squared_error: 0.1102 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1034\n",
            "Epoch 128/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0126 - root_mean_squared_error: 0.1101\n",
            "Epoch 128: val_loss improved from 0.01103 to 0.01080, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_128_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 170ms/step - loss: 0.0126 - root_mean_squared_error: 0.1102 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1019\n",
            "Epoch 129/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.0126 - root_mean_squared_error: 0.1103\n",
            "Epoch 129: val_loss did not improve from 0.01080\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 168ms/step - loss: 0.0126 - root_mean_squared_error: 0.1103 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1029\n",
            "Epoch 130/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0126 - root_mean_squared_error: 0.1102\n",
            "Epoch 130: val_loss did not improve from 0.01080\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 166ms/step - loss: 0.0126 - root_mean_squared_error: 0.1102 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1035\n",
            "Epoch 131/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0126 - root_mean_squared_error: 0.1099\n",
            "Epoch 131: val_loss did not improve from 0.01080\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 165ms/step - loss: 0.0126 - root_mean_squared_error: 0.1099 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1032\n",
            "Epoch 132/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0125 - root_mean_squared_error: 0.1096\n",
            "Epoch 132: val_loss did not improve from 0.01080\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 166ms/step - loss: 0.0125 - root_mean_squared_error: 0.1096 - val_loss: 0.0109 - val_root_mean_squared_error: 0.1025\n",
            "Epoch 133/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 0.0125 - root_mean_squared_error: 0.1099\n",
            "Epoch 133: val_loss did not improve from 0.01080\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 164ms/step - loss: 0.0125 - root_mean_squared_error: 0.1099 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1027\n",
            "Epoch 134/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 0.0126 - root_mean_squared_error: 0.1099\n",
            "Epoch 134: val_loss did not improve from 0.01080\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 164ms/step - loss: 0.0126 - root_mean_squared_error: 0.1099 - val_loss: 0.0109 - val_root_mean_squared_error: 0.1023\n",
            "Epoch 135/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - loss: 0.0124 - root_mean_squared_error: 0.1092\n",
            "Epoch 135: val_loss did not improve from 0.01080\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 175ms/step - loss: 0.0124 - root_mean_squared_error: 0.1092 - val_loss: 0.0109 - val_root_mean_squared_error: 0.1026\n",
            "Epoch 136/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0125 - root_mean_squared_error: 0.1096\n",
            "Epoch 136: val_loss did not improve from 0.01080\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 165ms/step - loss: 0.0125 - root_mean_squared_error: 0.1096 - val_loss: 0.0109 - val_root_mean_squared_error: 0.1026\n",
            "Epoch 137/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.0126 - root_mean_squared_error: 0.1101\n",
            "Epoch 137: val_loss did not improve from 0.01080\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 167ms/step - loss: 0.0126 - root_mean_squared_error: 0.1101 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1031\n",
            "Epoch 138/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.0125 - root_mean_squared_error: 0.1094\n",
            "Epoch 138: val_loss improved from 0.01080 to 0.01074, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_138_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0125 - root_mean_squared_error: 0.1094 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1015\n",
            "Epoch 139/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.0124 - root_mean_squared_error: 0.1093\n",
            "Epoch 139: val_loss did not improve from 0.01074\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 167ms/step - loss: 0.0124 - root_mean_squared_error: 0.1093 - val_loss: 0.0109 - val_root_mean_squared_error: 0.1023\n",
            "Epoch 140/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.0125 - root_mean_squared_error: 0.1097\n",
            "Epoch 140: val_loss did not improve from 0.01074\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 168ms/step - loss: 0.0125 - root_mean_squared_error: 0.1097 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1018\n",
            "Epoch 141/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0125 - root_mean_squared_error: 0.1094\n",
            "Epoch 141: val_loss did not improve from 0.01074\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0125 - root_mean_squared_error: 0.1094 - val_loss: 0.0109 - val_root_mean_squared_error: 0.1023\n",
            "Epoch 142/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - loss: 0.0124 - root_mean_squared_error: 0.1090\n",
            "Epoch 142: val_loss did not improve from 0.01074\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 189ms/step - loss: 0.0124 - root_mean_squared_error: 0.1090 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1021\n",
            "Epoch 143/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.0125 - root_mean_squared_error: 0.1095\n",
            "Epoch 143: val_loss did not improve from 0.01074\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 168ms/step - loss: 0.0125 - root_mean_squared_error: 0.1095 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1019\n",
            "Epoch 144/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0124 - root_mean_squared_error: 0.1092\n",
            "Epoch 144: val_loss did not improve from 0.01074\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 167ms/step - loss: 0.0124 - root_mean_squared_error: 0.1092 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1035\n",
            "Epoch 145/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0124 - root_mean_squared_error: 0.1092\n",
            "Epoch 145: val_loss improved from 0.01074 to 0.01074, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_145_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 171ms/step - loss: 0.0124 - root_mean_squared_error: 0.1092 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1016\n",
            "Epoch 146/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 0.0123 - root_mean_squared_error: 0.1089\n",
            "Epoch 146: val_loss did not improve from 0.01074\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 170ms/step - loss: 0.0124 - root_mean_squared_error: 0.1089 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1032\n",
            "Epoch 147/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0124 - root_mean_squared_error: 0.1091\n",
            "Epoch 147: val_loss did not improve from 0.01074\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 169ms/step - loss: 0.0124 - root_mean_squared_error: 0.1091 - val_loss: 0.0109 - val_root_mean_squared_error: 0.1026\n",
            "Epoch 148/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0124 - root_mean_squared_error: 0.1093\n",
            "Epoch 148: val_loss did not improve from 0.01074\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 168ms/step - loss: 0.0124 - root_mean_squared_error: 0.1093 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1017\n",
            "Epoch 149/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - loss: 0.0123 - root_mean_squared_error: 0.1086\n",
            "Epoch 149: val_loss did not improve from 0.01074\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 177ms/step - loss: 0.0123 - root_mean_squared_error: 0.1086 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1020\n",
            "Epoch 150/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.0124 - root_mean_squared_error: 0.1094\n",
            "Epoch 150: val_loss did not improve from 0.01074\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 159ms/step - loss: 0.0124 - root_mean_squared_error: 0.1094 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1019\n",
            "Epoch 151/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.0122 - root_mean_squared_error: 0.1084\n",
            "Epoch 151: val_loss did not improve from 0.01074\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 159ms/step - loss: 0.0122 - root_mean_squared_error: 0.1084 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1030\n",
            "Epoch 152/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 0.0124 - root_mean_squared_error: 0.1091\n",
            "Epoch 152: val_loss did not improve from 0.01074\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 160ms/step - loss: 0.0124 - root_mean_squared_error: 0.1091 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1033\n",
            "Epoch 153/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.0123 - root_mean_squared_error: 0.1088\n",
            "Epoch 153: val_loss did not improve from 0.01074\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 159ms/step - loss: 0.0123 - root_mean_squared_error: 0.1088 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1018\n",
            "Epoch 154/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.0123 - root_mean_squared_error: 0.1086\n",
            "Epoch 154: val_loss did not improve from 0.01074\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 158ms/step - loss: 0.0123 - root_mean_squared_error: 0.1086 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1019\n",
            "Epoch 155/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.0124 - root_mean_squared_error: 0.1092\n",
            "Epoch 155: val_loss did not improve from 0.01074\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 158ms/step - loss: 0.0124 - root_mean_squared_error: 0.1092 - val_loss: 0.0109 - val_root_mean_squared_error: 0.1023\n",
            "Epoch 156/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.0123 - root_mean_squared_error: 0.1088\n",
            "Epoch 156: val_loss did not improve from 0.01074\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 158ms/step - loss: 0.0123 - root_mean_squared_error: 0.1088 - val_loss: 0.0109 - val_root_mean_squared_error: 0.1022\n",
            "Epoch 157/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.0124 - root_mean_squared_error: 0.1090\n",
            "Epoch 157: val_loss improved from 0.01074 to 0.01073, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_157_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 161ms/step - loss: 0.0124 - root_mean_squared_error: 0.1090 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1015\n",
            "Epoch 158/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.0123 - root_mean_squared_error: 0.1086\n",
            "Epoch 158: val_loss did not improve from 0.01073\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 158ms/step - loss: 0.0123 - root_mean_squared_error: 0.1086 - val_loss: 0.0109 - val_root_mean_squared_error: 0.1026\n",
            "Epoch 159/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.0123 - root_mean_squared_error: 0.1086\n",
            "Epoch 159: val_loss did not improve from 0.01073\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 159ms/step - loss: 0.0123 - root_mean_squared_error: 0.1086 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1021\n",
            "Epoch 160/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0123 - root_mean_squared_error: 0.1086\n",
            "Epoch 160: val_loss did not improve from 0.01073\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0123 - root_mean_squared_error: 0.1086 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1019\n",
            "Epoch 161/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.0123 - root_mean_squared_error: 0.1086\n",
            "Epoch 161: val_loss did not improve from 0.01073\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 159ms/step - loss: 0.0123 - root_mean_squared_error: 0.1086 - val_loss: 0.0109 - val_root_mean_squared_error: 0.1024\n",
            "Epoch 162/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 0.0122 - root_mean_squared_error: 0.1084\n",
            "Epoch 162: val_loss improved from 0.01073 to 0.01058, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_162_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 171ms/step - loss: 0.0122 - root_mean_squared_error: 0.1084 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1008\n",
            "Epoch 163/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.0123 - root_mean_squared_error: 0.1085\n",
            "Epoch 163: val_loss did not improve from 0.01058\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 158ms/step - loss: 0.0123 - root_mean_squared_error: 0.1085 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1013\n",
            "Epoch 164/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.0122 - root_mean_squared_error: 0.1083\n",
            "Epoch 164: val_loss did not improve from 0.01058\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 157ms/step - loss: 0.0122 - root_mean_squared_error: 0.1083 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1030\n",
            "Epoch 165/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0122 - root_mean_squared_error: 0.1084\n",
            "Epoch 165: val_loss did not improve from 0.01058\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0122 - root_mean_squared_error: 0.1084 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1014\n",
            "Epoch 166/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0123 - root_mean_squared_error: 0.1087\n",
            "Epoch 166: val_loss did not improve from 0.01058\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 165ms/step - loss: 0.0123 - root_mean_squared_error: 0.1087 - val_loss: 0.0109 - val_root_mean_squared_error: 0.1024\n",
            "Epoch 167/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 0.0123 - root_mean_squared_error: 0.1086\n",
            "Epoch 167: val_loss did not improve from 0.01058\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 163ms/step - loss: 0.0123 - root_mean_squared_error: 0.1086 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1017\n",
            "Epoch 168/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0121 - root_mean_squared_error: 0.1077\n",
            "Epoch 168: val_loss did not improve from 0.01058\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 156ms/step - loss: 0.0121 - root_mean_squared_error: 0.1077 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1029\n",
            "Epoch 169/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - loss: 0.0122 - root_mean_squared_error: 0.1080\n",
            "Epoch 169: val_loss did not improve from 0.01058\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 179ms/step - loss: 0.0122 - root_mean_squared_error: 0.1080 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1014\n",
            "Epoch 170/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0121 - root_mean_squared_error: 0.1076\n",
            "Epoch 170: val_loss did not improve from 0.01058\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0121 - root_mean_squared_error: 0.1076 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1018\n",
            "Epoch 171/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0122 - root_mean_squared_error: 0.1083\n",
            "Epoch 171: val_loss did not improve from 0.01058\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0122 - root_mean_squared_error: 0.1083 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1017\n",
            "Epoch 172/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0121 - root_mean_squared_error: 0.1079\n",
            "Epoch 172: val_loss did not improve from 0.01058\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 165ms/step - loss: 0.0121 - root_mean_squared_error: 0.1079 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1027\n",
            "Epoch 173/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0122 - root_mean_squared_error: 0.1083\n",
            "Epoch 173: val_loss improved from 0.01058 to 0.01057, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_173_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 167ms/step - loss: 0.0122 - root_mean_squared_error: 0.1083 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1007\n",
            "Epoch 174/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0121 - root_mean_squared_error: 0.1078\n",
            "Epoch 174: val_loss improved from 0.01057 to 0.01054, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_174_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 167ms/step - loss: 0.0121 - root_mean_squared_error: 0.1078 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1006\n",
            "Epoch 175/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 0.0122 - root_mean_squared_error: 0.1080\n",
            "Epoch 175: val_loss did not improve from 0.01054\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 163ms/step - loss: 0.0122 - root_mean_squared_error: 0.1080 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1010\n",
            "Epoch 176/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0121 - root_mean_squared_error: 0.1077\n",
            "Epoch 176: val_loss did not improve from 0.01054\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 167ms/step - loss: 0.0121 - root_mean_squared_error: 0.1077 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1014\n",
            "Epoch 177/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0121 - root_mean_squared_error: 0.1076\n",
            "Epoch 177: val_loss did not improve from 0.01054\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0121 - root_mean_squared_error: 0.1076 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1031\n",
            "Epoch 178/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0121 - root_mean_squared_error: 0.1079\n",
            "Epoch 178: val_loss did not improve from 0.01054\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0121 - root_mean_squared_error: 0.1079 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1014\n",
            "Epoch 179/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0120 - root_mean_squared_error: 0.1074\n",
            "Epoch 179: val_loss did not improve from 0.01054\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0120 - root_mean_squared_error: 0.1074 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1007\n",
            "Epoch 180/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.0120 - root_mean_squared_error: 0.1073\n",
            "Epoch 180: val_loss did not improve from 0.01054\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 158ms/step - loss: 0.0120 - root_mean_squared_error: 0.1073 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1006\n",
            "Epoch 181/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0120 - root_mean_squared_error: 0.1073\n",
            "Epoch 181: val_loss did not improve from 0.01054\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0120 - root_mean_squared_error: 0.1073 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1017\n",
            "Epoch 182/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0121 - root_mean_squared_error: 0.1077\n",
            "Epoch 182: val_loss did not improve from 0.01054\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0121 - root_mean_squared_error: 0.1077 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1010\n",
            "Epoch 183/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0120 - root_mean_squared_error: 0.1071\n",
            "Epoch 183: val_loss did not improve from 0.01054\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0120 - root_mean_squared_error: 0.1071 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1031\n",
            "Epoch 184/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0120 - root_mean_squared_error: 0.1074\n",
            "Epoch 184: val_loss improved from 0.01054 to 0.01046, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_184_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 159ms/step - loss: 0.0120 - root_mean_squared_error: 0.1074 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1002\n",
            "Epoch 185/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.0120 - root_mean_squared_error: 0.1074\n",
            "Epoch 185: val_loss did not improve from 0.01046\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 158ms/step - loss: 0.0120 - root_mean_squared_error: 0.1074 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1006\n",
            "Epoch 186/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.0122 - root_mean_squared_error: 0.1080\n",
            "Epoch 186: val_loss improved from 0.01046 to 0.01039, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_186_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 160ms/step - loss: 0.0122 - root_mean_squared_error: 0.1080 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0998\n",
            "Epoch 187/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0120 - root_mean_squared_error: 0.1072\n",
            "Epoch 187: val_loss did not improve from 0.01039\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0120 - root_mean_squared_error: 0.1072 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1005\n",
            "Epoch 188/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.0120 - root_mean_squared_error: 0.1071\n",
            "Epoch 188: val_loss did not improve from 0.01039\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 158ms/step - loss: 0.0120 - root_mean_squared_error: 0.1071 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1025\n",
            "Epoch 189/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0119 - root_mean_squared_error: 0.1069\n",
            "Epoch 189: val_loss did not improve from 0.01039\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0119 - root_mean_squared_error: 0.1069 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1016\n",
            "Epoch 190/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.0119 - root_mean_squared_error: 0.1070\n",
            "Epoch 190: val_loss did not improve from 0.01039\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 158ms/step - loss: 0.0119 - root_mean_squared_error: 0.1070 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1006\n",
            "Epoch 191/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0120 - root_mean_squared_error: 0.1074\n",
            "Epoch 191: val_loss did not improve from 0.01039\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 157ms/step - loss: 0.0120 - root_mean_squared_error: 0.1074 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1012\n",
            "Epoch 192/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0119 - root_mean_squared_error: 0.1070\n",
            "Epoch 192: val_loss did not improve from 0.01039\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0119 - root_mean_squared_error: 0.1070 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1008\n",
            "Epoch 193/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0120 - root_mean_squared_error: 0.1071\n",
            "Epoch 193: val_loss did not improve from 0.01039\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0120 - root_mean_squared_error: 0.1071 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1002\n",
            "Epoch 194/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0119 - root_mean_squared_error: 0.1070\n",
            "Epoch 194: val_loss improved from 0.01039 to 0.01034, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_194_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 158ms/step - loss: 0.0119 - root_mean_squared_error: 0.1069 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0997\n",
            "Epoch 195/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0120 - root_mean_squared_error: 0.1072\n",
            "Epoch 195: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0120 - root_mean_squared_error: 0.1072 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1006\n",
            "Epoch 196/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0119 - root_mean_squared_error: 0.1069\n",
            "Epoch 196: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0119 - root_mean_squared_error: 0.1069 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0998\n",
            "Epoch 197/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0119 - root_mean_squared_error: 0.1067\n",
            "Epoch 197: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0119 - root_mean_squared_error: 0.1067 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1006\n",
            "Epoch 198/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0119 - root_mean_squared_error: 0.1069\n",
            "Epoch 198: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0119 - root_mean_squared_error: 0.1069 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0999\n",
            "Epoch 199/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0118 - root_mean_squared_error: 0.1063\n",
            "Epoch 199: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0118 - root_mean_squared_error: 0.1063 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1009\n",
            "Epoch 200/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.0119 - root_mean_squared_error: 0.1069\n",
            "Epoch 200: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 158ms/step - loss: 0.0119 - root_mean_squared_error: 0.1069 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1009\n",
            "Epoch 201/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0119 - root_mean_squared_error: 0.1067\n",
            "Epoch 201: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0119 - root_mean_squared_error: 0.1067 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1006\n",
            "Epoch 202/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0118 - root_mean_squared_error: 0.1063\n",
            "Epoch 202: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0118 - root_mean_squared_error: 0.1063 - val_loss: 0.0104 - val_root_mean_squared_error: 0.1001\n",
            "Epoch 203/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0119 - root_mean_squared_error: 0.1070\n",
            "Epoch 203: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0119 - root_mean_squared_error: 0.1070 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1003\n",
            "Epoch 204/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0119 - root_mean_squared_error: 0.1069\n",
            "Epoch 204: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0119 - root_mean_squared_error: 0.1069 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1008\n",
            "Epoch 205/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0118 - root_mean_squared_error: 0.1062\n",
            "Epoch 205: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 158ms/step - loss: 0.0118 - root_mean_squared_error: 0.1062 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0999\n",
            "Epoch 206/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 0.0118 - root_mean_squared_error: 0.1064\n",
            "Epoch 206: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 159ms/step - loss: 0.0118 - root_mean_squared_error: 0.1064 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1009\n",
            "Epoch 207/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.0119 - root_mean_squared_error: 0.1067\n",
            "Epoch 207: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 158ms/step - loss: 0.0119 - root_mean_squared_error: 0.1067 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1002\n",
            "Epoch 208/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0119 - root_mean_squared_error: 0.1066\n",
            "Epoch 208: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0119 - root_mean_squared_error: 0.1066 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0998\n",
            "Epoch 209/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.0118 - root_mean_squared_error: 0.1061\n",
            "Epoch 209: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 158ms/step - loss: 0.0118 - root_mean_squared_error: 0.1061 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1003\n",
            "Epoch 210/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0118 - root_mean_squared_error: 0.1062\n",
            "Epoch 210: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 167ms/step - loss: 0.0118 - root_mean_squared_error: 0.1062 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1016\n",
            "Epoch 211/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0119 - root_mean_squared_error: 0.1070\n",
            "Epoch 211: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0119 - root_mean_squared_error: 0.1070 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0997\n",
            "Epoch 212/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0118 - root_mean_squared_error: 0.1063\n",
            "Epoch 212: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0118 - root_mean_squared_error: 0.1063 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1005\n",
            "Epoch 213/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0118 - root_mean_squared_error: 0.1064\n",
            "Epoch 213: val_loss improved from 0.01034 to 0.01034, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_213_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 158ms/step - loss: 0.0118 - root_mean_squared_error: 0.1064 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0996\n",
            "Epoch 214/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0118 - root_mean_squared_error: 0.1061\n",
            "Epoch 214: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0118 - root_mean_squared_error: 0.1061 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1007\n",
            "Epoch 215/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0117 - root_mean_squared_error: 0.1060\n",
            "Epoch 215: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0117 - root_mean_squared_error: 0.1060 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1006\n",
            "Epoch 216/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.0118 - root_mean_squared_error: 0.1065\n",
            "Epoch 216: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 158ms/step - loss: 0.0118 - root_mean_squared_error: 0.1065 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0997\n",
            "Epoch 217/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 0.0118 - root_mean_squared_error: 0.1061\n",
            "Epoch 217: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 168ms/step - loss: 0.0118 - root_mean_squared_error: 0.1061 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1004\n",
            "Epoch 218/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.0117 - root_mean_squared_error: 0.1059\n",
            "Epoch 218: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 158ms/step - loss: 0.0117 - root_mean_squared_error: 0.1059 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1011\n",
            "Epoch 219/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.0117 - root_mean_squared_error: 0.1059\n",
            "Epoch 219: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0117 - root_mean_squared_error: 0.1059 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1001\n",
            "Epoch 220/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0117 - root_mean_squared_error: 0.1056\n",
            "Epoch 220: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0117 - root_mean_squared_error: 0.1056 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1010\n",
            "Epoch 221/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0118 - root_mean_squared_error: 0.1063\n",
            "Epoch 221: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0118 - root_mean_squared_error: 0.1063 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1002\n",
            "Epoch 222/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0117 - root_mean_squared_error: 0.1058\n",
            "Epoch 222: val_loss did not improve from 0.01034\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0117 - root_mean_squared_error: 0.1058 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1009\n",
            "Epoch 223/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.0117 - root_mean_squared_error: 0.1061\n",
            "Epoch 223: val_loss improved from 0.01034 to 0.01033, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_223_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 160ms/step - loss: 0.0117 - root_mean_squared_error: 0.1061 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0996\n",
            "Epoch 224/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.0117 - root_mean_squared_error: 0.1057\n",
            "Epoch 224: val_loss did not improve from 0.01033\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 159ms/step - loss: 0.0117 - root_mean_squared_error: 0.1057 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1002\n",
            "Epoch 225/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 0.0117 - root_mean_squared_error: 0.1061\n",
            "Epoch 225: val_loss did not improve from 0.01033\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 160ms/step - loss: 0.0117 - root_mean_squared_error: 0.1061 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1007\n",
            "Epoch 226/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.0117 - root_mean_squared_error: 0.1057\n",
            "Epoch 226: val_loss did not improve from 0.01033\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 159ms/step - loss: 0.0117 - root_mean_squared_error: 0.1057 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1016\n",
            "Epoch 227/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.0117 - root_mean_squared_error: 0.1058\n",
            "Epoch 227: val_loss improved from 0.01033 to 0.01022, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_227_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 160ms/step - loss: 0.0117 - root_mean_squared_error: 0.1058 - val_loss: 0.0102 - val_root_mean_squared_error: 0.0990\n",
            "Epoch 228/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.0117 - root_mean_squared_error: 0.1056\n",
            "Epoch 228: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 159ms/step - loss: 0.0117 - root_mean_squared_error: 0.1056 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1003\n",
            "Epoch 229/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.0117 - root_mean_squared_error: 0.1059\n",
            "Epoch 229: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 159ms/step - loss: 0.0117 - root_mean_squared_error: 0.1059 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0996\n",
            "Epoch 230/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.0117 - root_mean_squared_error: 0.1060\n",
            "Epoch 230: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 160ms/step - loss: 0.0117 - root_mean_squared_error: 0.1060 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0992\n",
            "Epoch 231/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.0116 - root_mean_squared_error: 0.1053\n",
            "Epoch 231: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 159ms/step - loss: 0.0116 - root_mean_squared_error: 0.1053 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1016\n",
            "Epoch 232/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 0.0117 - root_mean_squared_error: 0.1060\n",
            "Epoch 232: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 160ms/step - loss: 0.0117 - root_mean_squared_error: 0.1060 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1003\n",
            "Epoch 233/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0120 - root_mean_squared_error: 0.1072\n",
            "Epoch 233: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0120 - root_mean_squared_error: 0.1072 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0996\n",
            "Epoch 234/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0117 - root_mean_squared_error: 0.1058\n",
            "Epoch 234: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 154ms/step - loss: 0.0117 - root_mean_squared_error: 0.1058 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0995\n",
            "Epoch 235/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0116 - root_mean_squared_error: 0.1053\n",
            "Epoch 235: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 154ms/step - loss: 0.0116 - root_mean_squared_error: 0.1053 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0994\n",
            "Epoch 236/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0117 - root_mean_squared_error: 0.1060\n",
            "Epoch 236: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 153ms/step - loss: 0.0117 - root_mean_squared_error: 0.1060 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0995\n",
            "Epoch 237/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.0117 - root_mean_squared_error: 0.1057\n",
            "Epoch 237: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 154ms/step - loss: 0.0117 - root_mean_squared_error: 0.1057 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0994\n",
            "Epoch 238/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0116 - root_mean_squared_error: 0.1051\n",
            "Epoch 238: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 153ms/step - loss: 0.0116 - root_mean_squared_error: 0.1051 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0999\n",
            "Epoch 239/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0116 - root_mean_squared_error: 0.1055\n",
            "Epoch 239: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 154ms/step - loss: 0.0116 - root_mean_squared_error: 0.1055 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1010\n",
            "Epoch 240/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.0117 - root_mean_squared_error: 0.1056\n",
            "Epoch 240: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 155ms/step - loss: 0.0117 - root_mean_squared_error: 0.1056 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0995\n",
            "Epoch 241/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0115 - root_mean_squared_error: 0.1048\n",
            "Epoch 241: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 154ms/step - loss: 0.0115 - root_mean_squared_error: 0.1048 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0995\n",
            "Epoch 242/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0116 - root_mean_squared_error: 0.1053\n",
            "Epoch 242: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 153ms/step - loss: 0.0116 - root_mean_squared_error: 0.1053 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0992\n",
            "Epoch 243/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.0116 - root_mean_squared_error: 0.1052\n",
            "Epoch 243: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 154ms/step - loss: 0.0116 - root_mean_squared_error: 0.1052 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0998\n",
            "Epoch 244/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0116 - root_mean_squared_error: 0.1055\n",
            "Epoch 244: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 167ms/step - loss: 0.0116 - root_mean_squared_error: 0.1055 - val_loss: 0.0104 - val_root_mean_squared_error: 0.1001\n",
            "Epoch 245/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0117 - root_mean_squared_error: 0.1057\n",
            "Epoch 245: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 154ms/step - loss: 0.0117 - root_mean_squared_error: 0.1057 - val_loss: 0.0104 - val_root_mean_squared_error: 0.1000\n",
            "Epoch 246/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0116 - root_mean_squared_error: 0.1055\n",
            "Epoch 246: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 154ms/step - loss: 0.0116 - root_mean_squared_error: 0.1055 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0993\n",
            "Epoch 247/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.0116 - root_mean_squared_error: 0.1056\n",
            "Epoch 247: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 155ms/step - loss: 0.0116 - root_mean_squared_error: 0.1056 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0998\n",
            "Epoch 248/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0116 - root_mean_squared_error: 0.1052\n",
            "Epoch 248: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 154ms/step - loss: 0.0116 - root_mean_squared_error: 0.1052 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0996\n",
            "Epoch 249/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.0115 - root_mean_squared_error: 0.1050\n",
            "Epoch 249: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 155ms/step - loss: 0.0115 - root_mean_squared_error: 0.1050 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1004\n",
            "Epoch 250/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.0116 - root_mean_squared_error: 0.1053\n",
            "Epoch 250: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 155ms/step - loss: 0.0116 - root_mean_squared_error: 0.1053 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1004\n",
            "Epoch 251/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0115 - root_mean_squared_error: 0.1051\n",
            "Epoch 251: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 165ms/step - loss: 0.0115 - root_mean_squared_error: 0.1051 - val_loss: 0.0102 - val_root_mean_squared_error: 0.0991\n",
            "Epoch 252/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0115 - root_mean_squared_error: 0.1051\n",
            "Epoch 252: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0115 - root_mean_squared_error: 0.1051 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0998\n",
            "Epoch 253/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0116 - root_mean_squared_error: 0.1054\n",
            "Epoch 253: val_loss did not improve from 0.01022\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0116 - root_mean_squared_error: 0.1054 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0995\n",
            "Epoch 254/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0115 - root_mean_squared_error: 0.1049\n",
            "Epoch 254: val_loss improved from 0.01022 to 0.01018, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_254_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 158ms/step - loss: 0.0115 - root_mean_squared_error: 0.1049 - val_loss: 0.0102 - val_root_mean_squared_error: 0.0988\n",
            "Epoch 255/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0115 - root_mean_squared_error: 0.1047\n",
            "Epoch 255: val_loss did not improve from 0.01018\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0115 - root_mean_squared_error: 0.1047 - val_loss: 0.0102 - val_root_mean_squared_error: 0.0989\n",
            "Epoch 256/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0115 - root_mean_squared_error: 0.1047\n",
            "Epoch 256: val_loss did not improve from 0.01018\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0115 - root_mean_squared_error: 0.1047 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1010\n",
            "Epoch 257/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0115 - root_mean_squared_error: 0.1050\n",
            "Epoch 257: val_loss did not improve from 0.01018\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0115 - root_mean_squared_error: 0.1050 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0996\n",
            "Epoch 258/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0116 - root_mean_squared_error: 0.1051\n",
            "Epoch 258: val_loss did not improve from 0.01018\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0116 - root_mean_squared_error: 0.1051 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0997\n",
            "Epoch 259/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0115 - root_mean_squared_error: 0.1051\n",
            "Epoch 259: val_loss did not improve from 0.01018\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0115 - root_mean_squared_error: 0.1051 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0993\n",
            "Epoch 260/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0114 - root_mean_squared_error: 0.1046\n",
            "Epoch 260: val_loss did not improve from 0.01018\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0114 - root_mean_squared_error: 0.1046 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0999\n",
            "Epoch 261/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0115 - root_mean_squared_error: 0.1049\n",
            "Epoch 261: val_loss did not improve from 0.01018\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0115 - root_mean_squared_error: 0.1049 - val_loss: 0.0102 - val_root_mean_squared_error: 0.0989\n",
            "Epoch 262/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0115 - root_mean_squared_error: 0.1050\n",
            "Epoch 262: val_loss did not improve from 0.01018\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 157ms/step - loss: 0.0115 - root_mean_squared_error: 0.1050 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0998\n",
            "Epoch 263/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.0115 - root_mean_squared_error: 0.1049\n",
            "Epoch 263: val_loss did not improve from 0.01018\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 156ms/step - loss: 0.0115 - root_mean_squared_error: 0.1049 - val_loss: 0.0102 - val_root_mean_squared_error: 0.0989\n",
            "Epoch 264/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.0115 - root_mean_squared_error: 0.1047\n",
            "Epoch 264: val_loss did not improve from 0.01018\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 155ms/step - loss: 0.0115 - root_mean_squared_error: 0.1047 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0992\n",
            "Epoch 265/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0115 - root_mean_squared_error: 0.1049\n",
            "Epoch 265: val_loss did not improve from 0.01018\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 154ms/step - loss: 0.0115 - root_mean_squared_error: 0.1049 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1000\n",
            "Epoch 266/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0115 - root_mean_squared_error: 0.1048\n",
            "Epoch 266: val_loss did not improve from 0.01018\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 153ms/step - loss: 0.0115 - root_mean_squared_error: 0.1048 - val_loss: 0.0102 - val_root_mean_squared_error: 0.0991\n",
            "Epoch 267/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0114 - root_mean_squared_error: 0.1045\n",
            "Epoch 267: val_loss did not improve from 0.01018\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 154ms/step - loss: 0.0114 - root_mean_squared_error: 0.1045 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0992\n",
            "Epoch 268/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0114 - root_mean_squared_error: 0.1045\n",
            "Epoch 268: val_loss improved from 0.01018 to 0.01010, saving model to /content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_268_val_loss_0.01.keras\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 155ms/step - loss: 0.0114 - root_mean_squared_error: 0.1045 - val_loss: 0.0101 - val_root_mean_squared_error: 0.0983\n",
            "Epoch 269/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0115 - root_mean_squared_error: 0.1048\n",
            "Epoch 269: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 154ms/step - loss: 0.0115 - root_mean_squared_error: 0.1048 - val_loss: 0.0102 - val_root_mean_squared_error: 0.0986\n",
            "Epoch 270/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.0114 - root_mean_squared_error: 0.1043\n",
            "Epoch 270: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 155ms/step - loss: 0.0114 - root_mean_squared_error: 0.1043 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0999\n",
            "Epoch 271/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0115 - root_mean_squared_error: 0.1046\n",
            "Epoch 271: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 154ms/step - loss: 0.0115 - root_mean_squared_error: 0.1046 - val_loss: 0.0101 - val_root_mean_squared_error: 0.0986\n",
            "Epoch 272/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0113 - root_mean_squared_error: 0.1041\n",
            "Epoch 272: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 153ms/step - loss: 0.0113 - root_mean_squared_error: 0.1041 - val_loss: 0.0102 - val_root_mean_squared_error: 0.0988\n",
            "Epoch 273/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0115 - root_mean_squared_error: 0.1049\n",
            "Epoch 273: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 153ms/step - loss: 0.0115 - root_mean_squared_error: 0.1049 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1003\n",
            "Epoch 274/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0114 - root_mean_squared_error: 0.1046\n",
            "Epoch 274: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 152ms/step - loss: 0.0114 - root_mean_squared_error: 0.1046 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0994\n",
            "Epoch 275/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0113 - root_mean_squared_error: 0.1039\n",
            "Epoch 275: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 154ms/step - loss: 0.0113 - root_mean_squared_error: 0.1039 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1007\n",
            "Epoch 276/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.0114 - root_mean_squared_error: 0.1046\n",
            "Epoch 276: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 154ms/step - loss: 0.0114 - root_mean_squared_error: 0.1046 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0995\n",
            "Epoch 277/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0114 - root_mean_squared_error: 0.1045\n",
            "Epoch 277: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 153ms/step - loss: 0.0114 - root_mean_squared_error: 0.1045 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0997\n",
            "Epoch 278/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.0114 - root_mean_squared_error: 0.1044\n",
            "Epoch 278: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 154ms/step - loss: 0.0114 - root_mean_squared_error: 0.1044 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0996\n",
            "Epoch 279/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0113 - root_mean_squared_error: 0.1040\n",
            "Epoch 279: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 152ms/step - loss: 0.0113 - root_mean_squared_error: 0.1040 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0999\n",
            "Epoch 280/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0115 - root_mean_squared_error: 0.1047\n",
            "Epoch 280: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 152ms/step - loss: 0.0115 - root_mean_squared_error: 0.1047 - val_loss: 0.0102 - val_root_mean_squared_error: 0.0988\n",
            "Epoch 281/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 0.0113 - root_mean_squared_error: 0.1040\n",
            "Epoch 281: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 151ms/step - loss: 0.0113 - root_mean_squared_error: 0.1040 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0992\n",
            "Epoch 282/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0113 - root_mean_squared_error: 0.1041\n",
            "Epoch 282: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 154ms/step - loss: 0.0113 - root_mean_squared_error: 0.1041 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0999\n",
            "Epoch 283/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 0.0114 - root_mean_squared_error: 0.1042\n",
            "Epoch 283: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 152ms/step - loss: 0.0114 - root_mean_squared_error: 0.1042 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0995\n",
            "Epoch 284/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0114 - root_mean_squared_error: 0.1044\n",
            "Epoch 284: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 153ms/step - loss: 0.0114 - root_mean_squared_error: 0.1044 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0992\n",
            "Epoch 285/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - loss: 0.0114 - root_mean_squared_error: 0.1044\n",
            "Epoch 285: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 172ms/step - loss: 0.0114 - root_mean_squared_error: 0.1044 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0993\n",
            "Epoch 286/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0113 - root_mean_squared_error: 0.1041\n",
            "Epoch 286: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 152ms/step - loss: 0.0113 - root_mean_squared_error: 0.1041 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0993\n",
            "Epoch 287/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.0114 - root_mean_squared_error: 0.1042\n",
            "Epoch 287: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 154ms/step - loss: 0.0114 - root_mean_squared_error: 0.1042 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0992\n",
            "Epoch 288/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0113 - root_mean_squared_error: 0.1041\n",
            "Epoch 288: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 154ms/step - loss: 0.0113 - root_mean_squared_error: 0.1041 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0998\n",
            "Epoch 289/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.0113 - root_mean_squared_error: 0.1039\n",
            "Epoch 289: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 155ms/step - loss: 0.0113 - root_mean_squared_error: 0.1039 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0996\n",
            "Epoch 290/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0112 - root_mean_squared_error: 0.1036\n",
            "Epoch 290: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 152ms/step - loss: 0.0112 - root_mean_squared_error: 0.1036 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0994\n",
            "Epoch 291/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0113 - root_mean_squared_error: 0.1041\n",
            "Epoch 291: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 153ms/step - loss: 0.0113 - root_mean_squared_error: 0.1041 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0994\n",
            "Epoch 292/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0113 - root_mean_squared_error: 0.1040\n",
            "Epoch 292: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 153ms/step - loss: 0.0113 - root_mean_squared_error: 0.1040 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0994\n",
            "Epoch 293/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.0113 - root_mean_squared_error: 0.1041\n",
            "Epoch 293: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 155ms/step - loss: 0.0113 - root_mean_squared_error: 0.1041 - val_loss: 0.0102 - val_root_mean_squared_error: 0.0989\n",
            "Epoch 294/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0113 - root_mean_squared_error: 0.1041\n",
            "Epoch 294: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 152ms/step - loss: 0.0113 - root_mean_squared_error: 0.1041 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0994\n",
            "Epoch 295/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.0112 - root_mean_squared_error: 0.1033\n",
            "Epoch 295: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 155ms/step - loss: 0.0112 - root_mean_squared_error: 0.1033 - val_loss: 0.0104 - val_root_mean_squared_error: 0.0999\n",
            "Epoch 296/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0113 - root_mean_squared_error: 0.1040\n",
            "Epoch 296: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 153ms/step - loss: 0.0113 - root_mean_squared_error: 0.1040 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0994\n",
            "Epoch 297/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0112 - root_mean_squared_error: 0.1036\n",
            "Epoch 297: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 154ms/step - loss: 0.0112 - root_mean_squared_error: 0.1036 - val_loss: 0.0102 - val_root_mean_squared_error: 0.0989\n",
            "Epoch 298/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.0113 - root_mean_squared_error: 0.1038\n",
            "Epoch 298: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 155ms/step - loss: 0.0113 - root_mean_squared_error: 0.1038 - val_loss: 0.0101 - val_root_mean_squared_error: 0.0985\n",
            "Epoch 299/300\n",
            "\u001b[1m307/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.0112 - root_mean_squared_error: 0.1036\n",
            "Epoch 299: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 153ms/step - loss: 0.0112 - root_mean_squared_error: 0.1036 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0996\n",
            "Epoch 300/300\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0112 - root_mean_squared_error: 0.1035\n",
            "Epoch 300: val_loss did not improve from 0.01010\n",
            "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 154ms/step - loss: 0.0112 - root_mean_squared_error: 0.1035 - val_loss: 0.0103 - val_root_mean_squared_error: 0.0996\n",
            "Restoring model weights from the end of the best epoch: 268.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and validation history"
      ],
      "metadata": {
        "id": "YHnwdBjV9YJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_file_path = '/content/drive/MyDrive/GalaxyZoo/vitb16_training_history.csv'\n",
        "training_history = pd.read_csv(history_file_path)\n",
        "\n",
        "train_metric = training_history['root_mean_squared_error']\n",
        "val_metric = training_history['val_root_mean_squared_error']\n",
        "epochs = range(1, len(train_metric) + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(epochs, train_metric, 'b', label='Training RMSE')\n",
        "plt.plot(epochs, val_metric, 'r', label='Validation RMSE')\n",
        "plt.title('ViT: Training and validation RMSE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('RMSE')\n",
        "plt.legend()\n",
        "plt.savefig('/content/drive/MyDrive/GalaxyZoo/Plots/vitb16_trainval.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y7ILK6AOr3kO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "outputId": "381f2af8-b84a-48a6-fdab-7513fde2aa56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Out of sample performance"
      ],
      "metadata": {
        "id": "m1uCzYk_r1Ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build latest model from saved weights\n",
        "vit = build_vit()\n",
        "vit.load_weights('/content/drive/MyDrive/GalaxyZoo/vitb16_checkpoints/vitb16_epoch_268_val_loss_0.01.keras')\n",
        "\n",
        "vit.compile(optimizer='adam',\n",
        "            loss = custom_loss,\n",
        "            metrics=[RootMeanSquaredError()])"
      ],
      "metadata": {
        "id": "86nFxDXpKUZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "test_loss, test_rmse = vit.evaluate(X_test, y_test)\n",
        "print('Test RMSE:', test_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98FqKWO0LJMr",
        "outputId": "8e6f7ee4-ac85-4a09-bdb1-fda2db2f560f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 0.0104 - root_mean_squared_error: 0.0996\n",
            "Test RMSE: 0.09950700402259827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to resume training from saved checkpoint if runtime disconnects"
      ],
      "metadata": {
        "id": "VT7pUxpkkbvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_latest_checkpoint(model, checkpoint_dir):\n",
        "    list_of_files = glob.glob(checkpoint_dir + '/*.keras')\n",
        "    if not list_of_files:  # No checkpoints exist\n",
        "        return model  # Return the original model if no checkpoints found\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)  # Find the latest checkpoint\n",
        "    print(f\"Loading weights from {latest_file}\")\n",
        "    model = model.load_weights(latest_file)\n",
        "    return model\n",
        "\n",
        "# Assuming `model` is your model instance\n",
        "vit = load_latest_checkpoint(vit, checkpoint_dir)"
      ],
      "metadata": {
        "id": "1daGzFsN11EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_initial_epoch_from_history(history_file):\n",
        "    try:\n",
        "        # Load the training history CSV file\n",
        "        history_df = pd.read_csv(history_file)\n",
        "        # The initial epoch should be one more than the last epoch in the history\n",
        "        initial_epoch = history_df.epoch.max() + 1\n",
        "        print(f\"Resuming training from epoch {initial_epoch}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load history from {history_file}. Starting from scratch. Error: {e}\")\n",
        "        initial_epoch = 0\n",
        "    return initial_epoch\n",
        "\n",
        "# Get the initial_epoch value\n",
        "initial_epoch = get_initial_epoch_from_history(history_file)"
      ],
      "metadata": {
        "id": "F-G84sHS-zt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit.compile(optimizer='adam',\n",
        "            loss = custom_loss,\n",
        "            metrics=[RootMeanSquaredError()])"
      ],
      "metadata": {
        "id": "G1QTWducDnl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = vit.fit(train_generator, validation_data=(X_val, y_val), epochs=300,\n",
        "                  callbacks=[checkpoint_callback, csv_logger, early_stopping_callback],\n",
        "                  initial_epoch = initial_epoch)"
      ],
      "metadata": {
        "id": "Fx0vF4FdA554"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}